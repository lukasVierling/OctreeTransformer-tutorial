{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Octree Transformer Hands on Tutorial"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's start with some basic imports...*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torch\n",
    "import k3d\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "import pytorch_lightning as pl \n",
    "from pytorch_lightning import loggers as pl_loggers\n",
    "\n",
    "from tqdm import tqdm\n",
    "from tqdm.auto import trange\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils.save_obj import save_obj"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Furthermore, we need some constants.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PADDING_VALUE = 0\n",
    "NUM_VOCAB = 3+1\n",
    "RESOLUTION = 64\n",
    "SPATIAL_DIM = 3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data from the ShapeNet data set. For the purpose of this tutroial, there are 10 shapes already voxelized stored in the \"data\" folder."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Octree Data Structure\n",
    "The Octree data structure is a hierarchical representation of 3D voxel data. It starts with a root node that encompasses the entire object. This root node is then recursively split into eight children, forming an octagonal subdivision. Each child node can further be divided into eight children of its own, and this subdivision process continues until a certain resolution is reached. The resolution determines the maximum level of subdivision, and it impacts the number of leaf nodes in the Octree. Specifically, an Octree with a resolution of R has a maximum of 8^(ld(R))=RÂ³ leaf nodes. Leaf nodes represent the smallest subvolumes in the Octree, containing voxel data.\n",
    "\n",
    "![Octree Structure visualized](../images/octree_explained.png)\n",
    "\n",
    "<small>Picture by [The Infinite Loop](https://geidav.wordpress.com/2014/07/18/advanced-octrees-1-preliminaries-insertion-strategies-and-max-tree-depth/)</small>\n",
    "\n",
    "\n",
    "To optimize the Octree's efficiency, we utilize modifications to the traditional Octree structure. One key modification is the representation of subvolumes with the same value for all their voxels. In such cases, instead of representing each individual voxel, we can replace them with a single node that represents the common value. This pruning technique significantly reduces the number of nodes in the Octree, resulting in a more compact representation of the voxel data. \n",
    "\n",
    "Implementation Steps:\n",
    "\n",
    "    Determine the node value:\n",
    "        1: If all child elements are empty\n",
    "        2: If there are at least two child elements with different values\n",
    "        3: If all child elements are occupied\n",
    "    Prune the tree if the node value is 1 or 3:\n",
    "        Remove all child nodes of the current node and replace them with a single node representing the value of the current node.\n",
    "    Recursively split the tree if the node value is 2:\n",
    "        Create eight child nodes representing the subvolumes within the current node.\n",
    "        Repeat this splitting process for each child node until the desired resolution is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearize(array: np.ndarray,\n",
    "                    max_resolution: int = 8096):\n",
    "    def recursive_linearise(array: np.ndarray, pos: np.ndarray, dep: int = 1):\n",
    "        \"\"\" Recursive internal function to linearise given voxel array.\n",
    "\n",
    "        Note: Uses variables of parent function to store values.\n",
    "\n",
    "        Args:\n",
    "            array (np.ndarray): Numpy array (or subarray) holding pixels/voxels of a discretized shape.\n",
    "            dep (int, optional): Current recursion depth. Defaults to 0.\n",
    "        \"\"\"\n",
    "        # split input into an octree/quadtree\n",
    "        subarrays = split(array)\n",
    "        num_subarrays = len(subarrays)\n",
    "\n",
    "        # initialize dictionary only on first pass\n",
    "        if dep not in value:\n",
    "            value[dep] = []\n",
    "            depth[dep] = []\n",
    "            position[dep] = []\n",
    "\n",
    "        # compute values for each subarray\n",
    "        for idx, sub in enumerate(subarrays):\n",
    "            value[dep] += [1] if np.max(sub) == 0 else [3] if np.min(sub) > 0 else [2]\n",
    "            depth[dep] += [dep]\n",
    "            position[dep] += [2 * pos + dirs[idx]]\n",
    "\n",
    "        # process each subarray recursivelly\n",
    "        for idx, sub in enumerate(subarrays):\n",
    "            cur_idx = -num_subarrays + idx\n",
    "            if value[dep][cur_idx] == 2 and dep < max_dep:\n",
    "                recursive_linearise(sub, position[dep][cur_idx], dep + 1)\n",
    "\n",
    "    # initialise memory\n",
    "    value = {}\n",
    "    depth = {}\n",
    "    position = {}\n",
    "    dirs = np.array(list(itertools.product([1, 2], repeat=array.ndim)))\n",
    "    init_pos = np.array(array.ndim * [0])\n",
    "    max_dep = int(math.log2(max_resolution))\n",
    "\n",
    "    # call function recursivelly\n",
    "    recursive_linearise(array, init_pos)\n",
    "\n",
    "    # flatten dictionaries\n",
    "    value = np.array(list(itertools.chain(*value.values())))\n",
    "    depth = np.array(list(itertools.chain(*depth.values())))\n",
    "    position = np.array(list(itertools.chain(*position.values())))\n",
    "\n",
    "    return value, depth, position"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to implement some further function..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split(array: np.ndarray) -> np.ndarray:\n",
    "    \"\"\" Splits the given array along each axis in half.\n",
    "\n",
    "    Args:\n",
    "        elements (np.ndarray): Numpy array of arbitary dimension.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Array of splited elements with an additional dimension along the first axis.\n",
    "    \"\"\"\n",
    "    ndim = array.ndim\n",
    "    array = np.expand_dims(array, axis=0)\n",
    "    for i in range(ndim, 0, -1):\n",
    "        array = np.concatenate(np.split(array, indices_or_sections=2, axis=i), axis=0)\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class kdTree():\n",
    "    \"\"\" Implements a kd-tree data structure for volumetric/spatial objects. Works with arrays of spatial data as well\n",
    "    as linearised token sequence representations.\n",
    "\n",
    "    This class allows to transform array with spatial elements into kd-trees, where k can by any natural number. Each\n",
    "    node represents mixed elements, which can be split in its branches. Each leaf represents a final element which is\n",
    "    either completly empty or completly occupied. These structure can be than linearized as a sequence of tokens, which\n",
    "    is equivalent to the kd-tree. In the same way, as arrays with spatial elements can be transformed into kd-trees,\n",
    "    token sequences can be transformed into kd-trees. This allows to seamlessly transform arrays of spatial data into\n",
    "    token sequences and vice versa.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\" Initializes the kd-tree for the right spatial dimensionality.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        # 3D -> spatial dim = 3\n",
    "        self.SPATIAL_DIM = 3\n",
    "        # array with directions for the child nodes (x,y,z)\n",
    "        self.dirs = np.array(list(itertools.product([1, 2], repeat=self.SPATIAL_DIM)))\n",
    "\n",
    "\n",
    "    def concat(self, array: np.ndarray) -> np.ndarray:\n",
    "        \"\"\" Concats elements of the array along each dimension, where each subarray is given in the first axis.\n",
    "\n",
    "        Args:\n",
    "            array (np.ndarray): Numpy array, holding subarrays in the first axis.\n",
    "\n",
    "        Return:\n",
    "            np.ndarray: Array of elements with concatenated subarrays along each axis.\n",
    "        \"\"\"\n",
    "        for i in range(1, self.SPATIAL_DIM + 1):\n",
    "            array = np.concatenate(np.split(array, indices_or_sections=2, axis=0), axis=i)\n",
    "            #remove first dimension\n",
    "        return np.squeeze(array, axis=0)\n",
    "\n",
    "    def insert_element_array(self, elements, max_depth=float('Inf'), depth=0, pos=None):\n",
    "        \"\"\" Inserts an array of element values which is converted into a kd-tree.\n",
    "\n",
    "        Args:\n",
    "            elements: A numpy array of element values, with the dimensionality of the kd-tree.\n",
    "            max_depth: The maximum depth of the resulting kd-tree. All nodes at `max_depth` are marked as final.\n",
    "            depth: The current depth of the kd-tree. Used to recursively define the tree depth.\n",
    "\n",
    "        Return:\n",
    "            The current node containing inserted values. The returned node should be the root node of the kd-tree.\n",
    "        \"\"\"\n",
    "        self.depth = depth\n",
    "        self.resolution = np.array(elements.shape[0])\n",
    "        self.final = True\n",
    "        self.pos = np.array(self.SPATIAL_DIM * [0]) if pos is None else pos\n",
    "        # '1' - all elements are empty\n",
    "        # '2' - elements are empty and occupied\n",
    "        # '3' - all elements are occupied\n",
    "        self.value = 1 if np.max(elements) == 0 else 3 if np.min(elements) > 0 else 2\n",
    "\n",
    "        # input has a resolution of 1 and cannot be splitt anymore\n",
    "        if self.resolution <= 1:\n",
    "            return self\n",
    "\n",
    "        # splitt only when elements are mixed and we are not at maximum depth\n",
    "        if self.value == 2 and depth <= max_depth:\n",
    "            self.final = False\n",
    "\n",
    "            # split elements into subarrays\n",
    "            sub_elements = split(elements)\n",
    "\n",
    "            # compute new positions for future nodes\n",
    "            # layerwise intertwined_positions\n",
    "            new_pos = [2 * self.pos + d for d in self.dirs]\n",
    "\n",
    "            # create child nodes\n",
    "            self.child_nodes = [\n",
    "                kdTree().insert_element_array(e, max_depth, depth + 1, p)\n",
    "                for e, p in zip(sub_elements, new_pos)\n",
    "            ]\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_element_array(self, depth=float('Inf'), mode='occupancy'):\n",
    "        \"\"\" Converts the kd-tree into an array of elements.\n",
    "\n",
    "        Args:\n",
    "            depth: Defines the maximum depth of the children nodes, of which the value will be returned in the array.\n",
    "\n",
    "        Return:\n",
    "            A numpy array with the dimensionality of the kd-tree, which hold values defined by `mode`.\n",
    "\n",
    "        \"\"\"\n",
    "        res = self.SPATIAL_DIM * [self.resolution]\n",
    "        if self.final or self.depth == depth:\n",
    "            # return empty array if all elements are empty\n",
    "            if self.value == 1:\n",
    "                return np.tile(0, res)\n",
    "            # else return value based on `mode`\n",
    "            elif mode == 'occupancy':\n",
    "                return np.tile(1, res)\n",
    "\n",
    "        return self.concat(np.array([node.get_element_array(depth, mode) for node in self.child_nodes]))\n",
    "\n",
    "    def insert_token_sequence(self, value, resolution, max_depth=float('Inf')):\n",
    "        \"\"\" Inserts a token sequence which is parsed into a kd-tree.\n",
    "\n",
    "        Args:\n",
    "            value: A token sequence representing a spatial object. The values should consist only of '1', '2' and '3'.\n",
    "                The sequence can be eiter a string or an array of strings or integers.\n",
    "            resolution: The resolution of the token sequence. This value should be a power of 2.\n",
    "            max_depth: The maximum depth up to which the token sequence will be parsed.\n",
    "\n",
    "        Return:\n",
    "            A node which represents the given token sequence. The returned node should be the root node of the kd-tree.\n",
    "        \"\"\"\n",
    "        # fail-fast: malformed input sequence\n",
    "        all_tokens_valid = all(str(c) in '123' for c in value)\n",
    "        if not all_tokens_valid:\n",
    "            raise ValueError(\n",
    "                \"ERROR: Input sequence consists of invalid tokens. Check token values and array type.\" +\n",
    "                f\"Valid tokens consist of 1 (white), 2 (mixed) and 3 (black). Sequence: {value}.\"\n",
    "            )\n",
    "\n",
    "        # initialize self\n",
    "        self.value = 0\n",
    "        self.depth = 0\n",
    "        self.pos = np.array(self.SPATIAL_DIM * [0])\n",
    "        self.resolution = np.array(resolution)\n",
    "        self.final = False\n",
    "\n",
    "        # initialize parser\n",
    "        depth = 1\n",
    "        final_layer = False\n",
    "        resolution = resolution // 2\n",
    "\n",
    "        # initialize first nodes\n",
    "        open_set = []\n",
    "        self.child_nodes = [kdTree() for _ in range(2**self.SPATIAL_DIM)]\n",
    "        open_set.extend(self.child_nodes)\n",
    "        node_counter = len(open_set)\n",
    "\n",
    "        # compute new positions for future nodes\n",
    "        # layerwise intertwined_positions\n",
    "        pos_set = [2 * self.pos + d for d in self.dirs]\n",
    "\n",
    "        while len(value) > 0 and depth <= max_depth and len(open_set) > 0:\n",
    "            # consume first token of sequence\n",
    "            head = int(value[0])\n",
    "            value = value[1:] if len(value) > 0 else value\n",
    "            node_counter -= 1\n",
    "\n",
    "            # get next node that should be populated\n",
    "            node = open_set.pop(0)\n",
    "\n",
    "            # assign values to node\n",
    "            node.value = head\n",
    "            node.depth = depth\n",
    "            node.pos = pos_set.pop(0)\n",
    "            node.resolution = np.array(resolution)\n",
    "\n",
    "            # final node:\n",
    "            # - head is '1' or '3', thus all elements have the same value\n",
    "            # - the resolution is 1, thus the elements cannot be split anymore\n",
    "            # - we are in the last depth layer, thus all nodes are final\n",
    "            node.final = head in (1, 3) or np.array_equal(resolution, [1]) or final_layer\n",
    "            if not node.final:\n",
    "                node.child_nodes = [kdTree() for _ in range(2**self.SPATIAL_DIM)]\n",
    "                open_set.extend(node.child_nodes)\n",
    "\n",
    "                # TODO: add 'intertwined' position encoding\n",
    "                # compute new positions for future nodes - center of all pixels\n",
    "                pos_set.extend([node.pos + node.resolution // 2 * d for d in self.dirs])\n",
    "\n",
    "            # update depth\n",
    "            if node_counter <= 0:\n",
    "                depth += 1\n",
    "                resolution = np.array(resolution // 2)\n",
    "                # return if the resolution becomes less than 1 - no visible elements\n",
    "                if resolution < 1:\n",
    "                    return self\n",
    "\n",
    "                #TODO delete?\n",
    "                node_counter = len(open_set)\n",
    "                # fail-fast: malformed input sequence\n",
    "                if len(value) < node_counter:\n",
    "                    # perform simple sequence repair by appending missing tokens\n",
    "                    value = np.append(value, [0 for _ in range(node_counter - len(value))])\n",
    "\n",
    "                if len(value) == node_counter:\n",
    "                    final_layer = True\n",
    "\n",
    "        return self\n",
    "\n",
    "    def get_token_sequence(self, depth=float('Inf'), return_depth=False, return_pos=False):\n",
    "        \"\"\" Returns a linearised sequence representation of the kd-tree.\n",
    "\n",
    "        Args:\n",
    "            depth: Defines the maximum depth of the nodes, up to which the tree is parsed.\n",
    "            return_depth: Selects if the corresponding depth sequence should be returned.\n",
    "            return_pos: Selects if the corresponding position sequence should be returned.\n",
    "\n",
    "        Return\n",
    "            A numpy array consisting of integer values representing the linearised kd-tree. Returns additionally the\n",
    "            corresponding depth and position sequence if specified in `return_depth` or `return_pos`. The values are\n",
    "            returned in the following order: (value, depth, position).\n",
    "        \"\"\"\n",
    "        seq_value = []\n",
    "        seq_depth = []\n",
    "        seq_pos = []\n",
    "        open_set = []\n",
    "\n",
    "        # start with root node\n",
    "        open_set.extend(self.child_nodes)\n",
    "\n",
    "        while len(open_set) > 0:\n",
    "            node = open_set.pop(0)\n",
    "\n",
    "            # reached sufficient depth - return sequence so far\n",
    "            if node.depth > depth:\n",
    "                break\n",
    "\n",
    "            seq_value += [node.value]\n",
    "            seq_depth += [node.depth]\n",
    "            seq_pos += [node.pos]\n",
    "\n",
    "            if not node.final:\n",
    "                open_set += node.child_nodes\n",
    "\n",
    "        seq_value = np.asarray(seq_value)\n",
    "        seq_depth = np.asarray(seq_depth)\n",
    "        seq_pos = np.asarray(seq_pos)\n",
    "\n",
    "        # output format depends in flags 'return_depth' and 'return_pos'\n",
    "        output = [seq_value]\n",
    "        if return_depth:\n",
    "            output += [seq_depth]\n",
    "        if return_pos:\n",
    "            output += [seq_pos]\n",
    "        return output\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Set\n",
    "To create a pytorch data set, we can inheret from the torch.utils.data.Dataset class. This class requires us to implement the __len__ and __getitem__ methods. The __len__ method returns the number of samples in the data set and the __getitem__ method returns a sample from the data set at a given index.\n",
    "In our case, before returning an item from the data set, we first have to create an Octree from the voxel data and linearize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeNet(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset class for handling ShapeNet voxel grid data.\n",
    "    \n",
    "    Args:\n",
    "        shape_dir (str): Directory containing the voxel grid files.\n",
    "    \n",
    "    Attributes:\n",
    "        shape_dir (str): Directory containing the voxel grid files.\n",
    "        file_list (list): List of filenames in the shape directory.\n",
    "        path (list): List of paths to individual shape files.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, shape_dir):\n",
    "        \"\"\"\n",
    "        Initialize the ShapeNet dataset.\n",
    "\n",
    "        Args:\n",
    "            shape_dir (str): Directory containing the voxel grid files.\n",
    "            transform (callable, optional): A function/transform to apply to each voxel grid.\n",
    "            target_transform (callable, optional): A function/transform to apply to each target.\n",
    "        \"\"\"\n",
    "        self.shape_dir = shape_dir\n",
    "        self.file_list = os.listdir(shape_dir)\n",
    "        self.path = [os.path.join(shape_dir, file) for file in self.file_list]\n",
    "        \n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Get the total number of voxel grids in the dataset.\n",
    "\n",
    "        Returns:\n",
    "            int: Total number of voxel grids.\n",
    "        \"\"\"\n",
    "        return len(self.path)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get the voxel grid data and its corresponding sequence representation.\n",
    "\n",
    "        Args:\n",
    "            idx (int): Index of the desired data sample.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray: Sequence representation of the voxel grid.\n",
    "        \"\"\"\n",
    "        # load the voxel grid\n",
    "        voxels = np.load(self.path[idx])\n",
    "        # linearize the grid with fast function to speed up training\n",
    "        seq = linearize(voxels)\n",
    "        return seq\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will implement a collate function that takes a batch of sequences with varying lengths, pads them to equal length, and transforms them into three tensors: one for values, one for depth, and one for position. This function is crucial for preprocessing our data before feeding it into a deep learning model.\n",
    "\n",
    "\n",
    "![Sequence](../images/sequence.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderOnlyCollate():\n",
    "    \"\"\" Creates a collate module, which pads batched sequences to equal length with the padding token '0'. \"\"\"\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        \"\"\" Pads and packs a list of samples for the 'encoder_only' architecture. \"\"\"\n",
    "        # pad batched sequences with '0' to same length\n",
    "        seq = pad_batch(batch)\n",
    "        # update : mÃ¼sste doch stimmen? das eine ist unser input gebatch das andere die labels nicht im batch oder seq[-1]\n",
    "        return seq, seq[0]\n",
    "\n",
    "def to_sequences(batch):\n",
    "    \"\"\" Transform a list on numpy arrays into sequences of pytorch tensors. \"\"\"\n",
    "    batch = [(torch.tensor(v), torch.tensor(d), torch.tensor(p)) for v, d, p in batch]\n",
    "\n",
    "    # unpack batched sequences\n",
    "    return zip(*batch)\n",
    "\n",
    "\n",
    "def pad_batch(batch):\n",
    "    \"\"\" Unpack batch and pad each sequence to a tensor of equal length. \"\"\"\n",
    "    val, dep, pos = to_sequences(batch)\n",
    "\n",
    "    # pad each sequence\n",
    "    val_pad = pad_sequence(val, batch_first=True, padding_value=PADDING_VALUE)\n",
    "    dep_pad = pad_sequence(dep, batch_first=True, padding_value=PADDING_VALUE)\n",
    "    pos_pad = pad_sequence(pos, batch_first=True, padding_value=PADDING_VALUE)\n",
    "\n",
    "    return val_pad, dep_pad, pos_pad"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Octree Transformer Architecture\n",
    "\n",
    "\n",
    "![Overview](../images/overview.png)\n",
    "\n",
    "In this section, we will delve into the architecture of the Octree Transformer model, which is composed of three key components that work collaboratively to process sequences with an octree structure. Let's explore each component in detail.\n",
    "\n",
    "## 1. Embedding Layer\n",
    "\n",
    "The embedding layer is the starting point of our Octree Transformer. Here, we embed both the input values and positions into a vector space. However, due to the exponential growth of linearized octrees, we need to employ compression techniques to manage computational complexity effectively.\n",
    "\n",
    "## 2. Transformer Stack\n",
    "\n",
    "The heart of our model lies within the Transformer Stack. This stack is comprised of multiple transformer blocks, each playing a crucial role in generating representations for our sequences. Each transformer block is made up of two pivotal components: the Multi-Head Attention layer and the Feed Forward layer. For further information on the transformer architecture we refer to the [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper.\n",
    "\n",
    "## 3. Generative Head\n",
    "\n",
    "The final phase of the Octree Transformer is the Generative Head, which is responsible for generating the output sequence. This head consists of two crucial components: the Linear layer and the Softmax layer.\n",
    "\n",
    "### Linear Layer\n",
    "The Linear layer functions as a bridge, mapping the output of the transformer stack to the vocabulary size. It enables the model to project the learned representations into a format compatible with the vocabulary.\n",
    "\n",
    "### Softmax Layer\n",
    "The Softmax layer takes the transformed output and generates a probability distribution over the vocabulary. By assigning probabilities to different tokens, the Softmax layer facilitates the generation of the final sequence.\n",
    "\n",
    "## Addressing Compression and Upsampling\n",
    "\n",
    "The Octree Transformer employs compression techniques to handle the expansion of linearized octrees, ensuring computational feasibility. Additionally, to counteract the effects of compression, certain layers of the octree require the Generative Head to generate multiple tokens. This challenge is tackled through upsampling methods like deconvolution, allowing the model to generate the necessary tokens for a given layer.\n",
    "\n",
    "<span style=\"font-style: italic;\">Now, let's get started!</span>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer\n",
    "\n",
    "When working with tokens, we have to use a prober embedding for the tokens. Furthermore, we will use learned positional encodings.\n",
    "The learned positional encoding represent the position of the token in the Octree. They are learned and can thereby be trained as part of the model. We create the positional encoding by **adding** the encoded value of the x,y,z coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Positional Encoding module for adding positional information to input sequences.\n",
    "    \n",
    "    Args:\n",
    "        resolution (int): The resolution of the 3D grid.\n",
    "        embed_dim (int): The dimensionality of the embedding.\n",
    "    \n",
    "    Attributes:\n",
    "        x_encoding (nn.Embedding): Embedding layer for x-coordinate positional encoding.\n",
    "        y_encoding (nn.Embedding): Embedding layer for y-coordinate positional encoding.\n",
    "        z_encoding (nn.Embedding): Embedding layer for z-coordinate positional encoding.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, resolution, embed_dim):\n",
    "        \"\"\"\n",
    "        Initialize the PositionalEncoding module.\n",
    "        \n",
    "        Args:\n",
    "            resolution (int): The resolution of the 3D grid.\n",
    "            embed_dim (int): The dimensionality of the embedding.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        # Using *2 because the sequence length can grow to 2*resolution\n",
    "        self.x_encoding = nn.Embedding(2 * resolution, embed_dim, padding_idx=PADDING_VALUE)\n",
    "        self.y_encoding = nn.Embedding(2 * resolution, embed_dim, padding_idx=PADDING_VALUE)\n",
    "        self.z_encoding = nn.Embedding(2 * resolution, embed_dim, padding_idx=PADDING_VALUE)\n",
    "\n",
    "    def forward(self, position):\n",
    "        \"\"\"\n",
    "        Apply positional encoding to the input position tensor.\n",
    "        \n",
    "        Args:\n",
    "            position (torch.Tensor): Input tensor containing position information.\n",
    "                Shape: (batch_size, sequence_length, 3)\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The positional embeddings for each position in the input tensor.\n",
    "                Shape: (batch_size, sequence_length, embed_dim)\n",
    "        \"\"\"\n",
    "        x = self.x_encoding(position[:, :, 0])  # Get x-coordinate embeddings\n",
    "        y = self.y_encoding(position[:, :, 1])  # Get y-coordinate embeddings\n",
    "        z = self.z_encoding(position[:, :, 2])  # Get z-coordinate embeddings\n",
    "        return x + y + z  # Combine embeddings for positional encoding\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can create the whole embedding layer for the input using the positional encoding layer and another **embedding for the values** of the nodes.\n",
    "Finally, we will add both outputs and return the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Embedding Layer combining value and positional embeddings.\n",
    "\n",
    "    Args:\n",
    "        num_vocab (int): Number of vocabulary items.\n",
    "        embed_dim (int): Dimensionality of the embeddings.\n",
    "        resolution (int): Resolution of the 3D grid.\n",
    "\n",
    "    Attributes:\n",
    "        positional_encoding (PositionalEncoding): Positional encoding module.\n",
    "        value_embedding (nn.Embedding): Embedding layer for value embeddings.\n",
    "        mask (torch.Tensor): Mask to handle padding values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_vocab, embed_dim, resolution):\n",
    "        \"\"\"\n",
    "        Initialize the EmbeddingLayer.\n",
    "\n",
    "        Args:\n",
    "            num_vocab (int): Number of vocabulary items.\n",
    "            embed_dim (int): Dimensionality of the embeddings.\n",
    "            resolution (int): Resolution of the 3D grid.\n",
    "        \"\"\"\n",
    "        super(EmbeddingLayer, self).__init__()\n",
    "        self.positional_encoding = PositionalEncoding(resolution, embed_dim)\n",
    "        self.value_embedding = nn.Embedding(num_vocab, embed_dim, padding_idx=PADDING_VALUE)\n",
    "\n",
    "    def forward(self, value, depth, position):\n",
    "        \"\"\"\n",
    "        Apply the embedding layer to the input value, depth, and position tensors.\n",
    "\n",
    "        Args:\n",
    "            value (torch.Tensor): Input tensor containing value indices.\n",
    "                Shape: (batch_size, sequence_length)\n",
    "            depth (torch.Tensor): Input tensor containing depth values.\n",
    "                Shape: (batch_size, sequence_length)\n",
    "            position (torch.Tensor): Input tensor containing position information.\n",
    "                Shape: (batch_size, sequence_length, 3)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Combined embeddings of positional and value information.\n",
    "                Shape: (batch_size, sequence_length, embed_dim)\n",
    "        \"\"\"\n",
    "        self.mask = (value == PADDING_VALUE)  # Create mask for padding values\n",
    "        pos = self.positional_encoding(position)  # Get positional embeddings\n",
    "        val = self.value_embedding(value)  # Get value embeddings\n",
    "        return pos + val  # Combine positional and value embeddings\n",
    "\n",
    "    def get_masks(self):\n",
    "        \"\"\"\n",
    "        Get the mask tensor used for handling padding values.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Mask tensor.\n",
    "        \"\"\"\n",
    "        return self.mask\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*There is still one problem left...*\n",
    "\n",
    "\n",
    "**The Challenge**: As the octree structure unfolds, the size of the linearized representation grows exponentially. This escalation in size can impede the model's efficiency, rendering it computationally burdensome, particularly in the lower levels of the octree.\n",
    "\n",
    "**The Solution**: Our remedy involves the application of convolutions across the embedded sequence. Convolutional operations allow us to effectively compress the information while retaining essential features and patterns. By concentrating on the lower octree levels, where the exponential growth is most pronounced, we strike a balance between efficient computation and information retention.\n",
    "\n",
    "**Convolutional** Compression: The essence of this technique lies in leveraging convolutional layers to extract essential features and patterns from the sequence of embedded tokens. By doing so, we attain a compressed representation that captures crucial information while alleviating the computational intensity associated with extensive linearized structures.\n",
    "\n",
    "\n",
    "![Compression](../images/compression.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Embedding module that combines positional and value embeddings\n",
    "    and applies 1D convolution on the embedded sequence.\n",
    "\n",
    "    Args:\n",
    "        num_vocab (int): Number of vocabulary items.\n",
    "        embed_dim (int): Dimensionality of the embeddings.\n",
    "        resolution (int): Resolution of the 3D grid.\n",
    "        conv_size (int): Kernel size for convolution.\n",
    "\n",
    "    Attributes:\n",
    "        num_vocab (int): Number of vocabulary items.\n",
    "        embed_dim (int): Dimensionality of the embeddings.\n",
    "        resolution (int): Resolution of the 3D grid.\n",
    "        chunk_size (int): Size of chunks for convolution.\n",
    "        embedding (EmbeddingLayer): Embedding layer for combining embeddings.\n",
    "        conv (nn.Conv1d): 1D convolutional layer.\n",
    "        mask (torch.Tensor): Mask to handle padding values.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_vocab, embed_dim, resolution, conv_size):\n",
    "        \"\"\"\n",
    "        Initialize the ConvolutionEmbedding module.\n",
    "\n",
    "        Args:\n",
    "            num_vocab (int): Number of vocabulary items.\n",
    "            embed_dim (int): Dimensionality of the embeddings.\n",
    "            resolution (int): Resolution of the 3D grid.\n",
    "            conv_size (int): Kernel size for convolution.\n",
    "        \"\"\"\n",
    "        super(ConvolutionEmbedding, self).__init__()\n",
    "        self.num_vocab = num_vocab\n",
    "        self.embed_dim = embed_dim\n",
    "        self.resolution = resolution\n",
    "        self.chunk_size = conv_size\n",
    "        self.embedding = EmbeddingLayer(num_vocab, embed_dim, resolution)\n",
    "        self.conv = nn.Conv1d(embed_dim, embed_dim, kernel_size=conv_size, stride=conv_size)\n",
    "\n",
    "    def forward(self, value, depth, position):\n",
    "        \"\"\"\n",
    "        Apply ConvolutionEmbedding to input value, depth, and position tensors.\n",
    "\n",
    "        Args:\n",
    "            value (torch.Tensor): Input tensor containing value indices.\n",
    "                Shape: (batch_size, sequence_length)\n",
    "            depth (torch.Tensor): Input tensor containing depth values.\n",
    "                Shape: (batch_size, sequence_length)\n",
    "            position (torch.Tensor): Input tensor containing position information.\n",
    "                Shape: (batch_size, sequence_length, 3)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Resulting tensor after applying convolution.\n",
    "                Shape: (batch_size, new_sequence_length, embed_dim)\n",
    "        \"\"\"\n",
    "        # create a mask in the same size of the resulting embedding (chunk_size steps)\n",
    "        self.mask = self.padding_mask(value[:, ::self.chunk_size])\n",
    "        # create embedding for sequence\n",
    "        embedded_seq = self.embedding(value, depth, position)\n",
    "        # apply convolution\n",
    "        return self.conv(embedded_seq.permute(0, 2, 1)).permute(0, 2, 1)\n",
    "\n",
    "    def padding_mask(self, value):\n",
    "        \"\"\"\n",
    "        Create a mask to handle padding values.\n",
    "\n",
    "        Args:\n",
    "            value (torch.Tensor): Input tensor containing value indices.\n",
    "                Shape: (batch_size, sequence_length)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Mask tensor for padding values.\n",
    "        \"\"\"\n",
    "        return value == PADDING_VALUE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we reach a critical point in our deep learning journey, we're focusing on creating embeddings that cover our entire sequence. This move is a significant step towards effectively utilizing octree structures. Our strategy involves using convolutional techniques tailored to each layer of the octree, which is a key aspect of our approach.\n",
    "\n",
    "**Essential Strategy**: At the core of our method is the idea of layer-specific embeddings. We recognize that different layers of the octree need different kinds of information. To address this, we're using convolutional embeddings customized for each layer.\n",
    "\n",
    "**Varied Information Gathering**: What makes our approach unique is its ability to gather information at different levels. This is achieved by applying different sizes of convolutional filters to each layer. These filters help us capture information while adapting to the specific details of each layer.\n",
    "\n",
    "**Step-by-Step Layer Embeddings**: Our process for creating these embeddings happens step by step, layer by layer. For each layer, we create a specific convolutional embedding that caters to that layer's specific information needs. This method ensures that every part of the octree sequence is well represented in the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SequenceEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Sequence Embedding module that combines different ConvolutionEmbedding layers.\n",
    "\n",
    "    Args:\n",
    "        num_vocab (int): Number of vocabulary items.\n",
    "        embed_dim (int): Dimensionality of the embeddings.\n",
    "        resolution (int): Resolution of the 3D grid.\n",
    "\n",
    "    Attributes:\n",
    "        embeddings (list): List of ConvolutionEmbedding instances for different layers.\n",
    "        mask (torch.Tensor): Padding mask for the sequence.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_vocab, embed_dim, resolution):\n",
    "        \"\"\"\n",
    "        Initialize the SequenceEmbedding module.\n",
    "\n",
    "        Args:\n",
    "            num_vocab (int): Number of vocabulary items.\n",
    "            embed_dim (int): Dimensionality of the embeddings.\n",
    "            resolution (int): Resolution of the 3D grid.\n",
    "        \"\"\"\n",
    "        super(SequenceEmbedding, self).__init__()\n",
    "        self.embeddings = [\n",
    "            ConvolutionEmbedding(num_vocab, embed_dim, resolution, conv_size=1),\n",
    "            ConvolutionEmbedding(num_vocab, embed_dim, resolution, conv_size=1),\n",
    "            ConvolutionEmbedding(num_vocab, embed_dim, resolution, conv_size=1),\n",
    "            ConvolutionEmbedding(num_vocab, embed_dim, resolution, conv_size=4),\n",
    "            ConvolutionEmbedding(num_vocab, embed_dim, resolution, conv_size=8),\n",
    "            ConvolutionEmbedding(num_vocab, embed_dim, resolution, conv_size=8),\n",
    "        ]\n",
    "\n",
    "    def forward(self, value, depth, position):\n",
    "        \"\"\"\n",
    "        Apply the SequenceEmbedding to input value, depth, and position tensors.\n",
    "\n",
    "        Args:\n",
    "            value (torch.Tensor): Input tensor containing value indices.\n",
    "                Shape: (batch_size, sequence_length)\n",
    "            depth (torch.Tensor): Input tensor containing depth values.\n",
    "                Shape: (batch_size, sequence_length)\n",
    "            position (torch.Tensor): Input tensor containing position information.\n",
    "                Shape: (batch_size, sequence_length, 3)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Padded sequence embedding tensor.\n",
    "                Shape: (batch_size, padded_sequence_length, embed_dim)\n",
    "        \"\"\"\n",
    "\n",
    "        padding_mask = []\n",
    "        # extract value, depth, and position sequence of the current sample\n",
    "        val, dep, pos = value[0], depth[0], position[0]\n",
    "        b_emb = torch.tensor([])\n",
    "\n",
    "        # embed layerwise\n",
    "        for layer_idx, embedding in enumerate(self.embeddings):\n",
    "            layer_depth = layer_idx + 1\n",
    "            # extract layer sequence\n",
    "            val_seq = val[dep == layer_depth]\n",
    "            dep_seq = dep[dep == layer_depth]\n",
    "            pos_seq = pos[dep == layer_depth]\n",
    "\n",
    "            if val_seq.shape[0] == 0:\n",
    "                break\n",
    "\n",
    "            # compute layer embedding\n",
    "            layer_emb = embedding(\n",
    "                val_seq.unsqueeze(0),\n",
    "                dep_seq.unsqueeze(0),\n",
    "                pos_seq.unsqueeze(0),\n",
    "            )[0]\n",
    "            # append layer embedding to embedding\n",
    "            b_emb = torch.cat([b_emb, layer_emb])\n",
    "\n",
    "        sequence = b_emb.unsqueeze(0)\n",
    "        # create padding mask\n",
    "        padding_mask = [torch.zeros(b_emb.shape[0], dtype=torch.bool)]\n",
    "        # pad the sequence with 1s so the transformer will not attend to these tokens\n",
    "        self.mask = pad_sequence(padding_mask, batch_first=True, padding_value=1)\n",
    "        # pad embedding sequence\n",
    "        return pad_sequence(sequence, batch_first=True, padding_value=0.0)\n",
    "\n",
    "    def get_mask(self):\n",
    "        \"\"\"\n",
    "        Get the padding mask for the sequence.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Padding mask, where padding tokens '0' of the value sequence are masked out.\n",
    "        \"\"\"\n",
    "        return self.mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def padding_mask(input_sequence):\n",
    "    \"\"\" Create a padding mask for the given input.\n",
    "\n",
    "        Always assumens '0' as a padding value. `input_sequence` has the shape (N, S).\n",
    "    \"\"\"\n",
    "    return torch.zeros_like(input_sequence).masked_fill(input_sequence == 0, 1).bool()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generative Head"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the time being, we'll set aside the transformer aspect and direct our attention to the generative head. The concept here shares **similarity with the embedding process**, albeit in the opposite direction. We initiate the process with a linear head, incorporating a GELU activation function followed by a linear layer. To augment the expressive capabilities of this head, we introduce positional encoding. This integration of positional encoding serves to enhance the model's ability to understand and generate contextually meaningful outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear Head module for generating output tokens.\n",
    "\n",
    "    Args:\n",
    "        num_vocab (int): Number of vocabulary items.\n",
    "        embed_dim (int): Dimensionality of the embeddings.\n",
    "        resolution (int): Resolution of the 3D grid.\n",
    "\n",
    "    Attributes:\n",
    "        linear (nn.Linear): Linear layer for token generation.\n",
    "        pos_enc (PositionalEncoding): Positional encoding module.\n",
    "        activation (nn.GELU): Activation function for intermediate processing.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_vocab, embed_dim, resolution):\n",
    "        \"\"\"\n",
    "        Initialize the LinearHead module.\n",
    "\n",
    "        Args:\n",
    "            num_vocab (int): Number of vocabulary items.\n",
    "            embed_dim (int): Dimensionality of the embeddings.\n",
    "            resolution (int): Resolution of the 3D grid.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(embed_dim, num_vocab)\n",
    "        self.pos_enc = PositionalEncoding(resolution, num_vocab)\n",
    "        self.activation = nn.GELU()\n",
    "\n",
    "    def forward(self, x, value, depth, pos):\n",
    "        \"\"\"\n",
    "        Apply LinearHead to input data.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            value (torch.Tensor): Input tensor containing value indices.\n",
    "                Shape: (batch_size, sequence_length)\n",
    "            depth (torch.Tensor): Input tensor containing depth values.\n",
    "                Shape: (batch_size, sequence_length)\n",
    "            pos (torch.Tensor): Input tensor containing position information.\n",
    "                Shape: (batch_size, sequence_length, 3)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Generated token tensor.\n",
    "        \"\"\"\n",
    "        x = self.activation(x)  # Apply activation function\n",
    "        x = self.linear(x)  # Apply linear transformation\n",
    "        pos_enc = self.pos_enc(pos)  # Get positional encoding\n",
    "        x = x + pos_enc  # Add positional encoding\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As previously explained, we need to **reverse** the compression applied in the sequence embedding. To achieve this, we employ a convolutional head that facilitates **deconvolutions** for upsampling the input to a specified conv_size. In order to enhance this upsampling procedure with extra contextual information, we leverage the sequence embedding by passing it through a convolutional layer. This approach integrates the benefits of both upsampling and embedding, contributing to the overall effectiveness of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Head module for generating output tokens using convolutional operations.\n",
    "\n",
    "    Args:\n",
    "        num_vocab (int): Number of vocabulary items.\n",
    "        embed_dim (int): Dimensionality of the embeddings.\n",
    "        head_dim (int): Dimensionality of the intermediate representation.\n",
    "        resolution (int): Resolution of the 3D grid.\n",
    "        conv_size (int): Kernel size for convolution.\n",
    "\n",
    "    Attributes:\n",
    "        activation (nn.GELU): Activation function for intermediate processing.\n",
    "        deconvolution (nn.ConvTranspose1d): Transposed convolution layer.\n",
    "        convolution (BlockConvolution): BlockConvolution layer for convolution.\n",
    "        embed (EmbeddingLayer): Embedding layer for combining embeddings.\n",
    "        linear (nn.Linear): Linear layer for token generation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_vocab, embed_dim, head_dim, resolution, conv_size):\n",
    "        \"\"\"\n",
    "        Initialize the ConvolutionalHead module.\n",
    "\n",
    "        Args:\n",
    "            num_vocab (int): Number of vocabulary items.\n",
    "            embed_dim (int): Dimensionality of the embeddings.\n",
    "            head_dim (int): Dimensionality of the intermediate representation.\n",
    "            resolution (int): Resolution of the 3D grid.\n",
    "            conv_size (int): Kernel size for convolution.\n",
    "        \"\"\"\n",
    "        super(ConvolutionalHead, self).__init__()\n",
    "        self.activation = nn.GELU()\n",
    "        self.deconvolution = nn.ConvTranspose1d(embed_dim, head_dim, conv_size, stride=conv_size)\n",
    "        self.convolution = BlockConvolution(head_dim, head_dim, conv_size)\n",
    "        self.embed = EmbeddingLayer(num_vocab, head_dim, resolution)\n",
    "        self.linear = nn.Linear(head_dim, num_vocab)\n",
    "\n",
    "    def forward(self, x, value, depth, pos):\n",
    "        \"\"\"\n",
    "        Apply ConvolutionalHead to input data.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Input tensor.\n",
    "            value (torch.Tensor): Input tensor containing value indices.\n",
    "                Shape: (batch_size, sequence_length)\n",
    "            depth (torch.Tensor): Input tensor containing depth values.\n",
    "                Shape: (batch_size, sequence_length)\n",
    "            pos (torch.Tensor): Input tensor containing position information.\n",
    "                Shape: (batch_size, sequence_length, 3)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Generated token tensor.\n",
    "        \"\"\"\n",
    "        x = self.activation(x)  # Apply activation function\n",
    "        x = self.deconvolution(x.transpose(1, 2)).transpose(1, 2)  # Apply deconvolution, transpoe dimesnions to convolute over the featrues\n",
    "        embed = self.embed(value, depth, pos)  # Get embedded representation\n",
    "        embed = self.activation(x)  # Apply activation function\n",
    "        embed = self.convolution(embed[:, :x.shape[1]])  # Apply convolution\n",
    "        x = x + embed  # Add convolution output to the deconvolution output\n",
    "        x = self.activation(x)  # Apply activation function\n",
    "        return self.linear(x)  # Generate output tokens using linear layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Block Convolution, we perform a 1D convolution operation on the input and advance features in a block-wise manner, unraveling insights block by block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockConvolution(nn.Module):\n",
    "    def __init__(self, source_dim, target_dim, block_size):\n",
    "        \"\"\" Performs masked blockwise convolution on an input sequence.\n",
    "            The mask is always an upper right triangle matrix with zeros on the diagonal.\n",
    "\n",
    "        Args:\n",
    "            source_dim: Defines the embedding dimension of the input sequence.\n",
    "            target_dim: Defines the embedding dimension of the output sequence.\n",
    "            block_size: Defines the size of the block over which we convolute.\n",
    "        \"\"\"\n",
    "        super(BlockConvolution, self).__init__()\n",
    "\n",
    "        self.block_size = block_size\n",
    "        self.convolution = nn.Conv1d(source_dim, target_dim, (1,), bias=False)\n",
    "        sigma = math.sqrt(1. / (block_size * source_dim))\n",
    "        self.bias = nn.Parameter(torch.empty(block_size))\n",
    "        nn.init.uniform_(self.bias, -sigma, sigma)\n",
    "\n",
    "    def forward(self, seq_vector):\n",
    "        \"\"\" Convolute tokens to reduce sequence length\n",
    "\n",
    "        Args:\n",
    "            seq_vector: Sequence vector with elements of the shape [N, S, E].\n",
    "\n",
    "        Return:\n",
    "            Sequence vector with the same length and target embedding dimension [N, S, E']\n",
    "        \"\"\"\n",
    "        #basic convolution with some transpose for dimension fit\n",
    "        features = self.convolution(seq_vector.transpose(1, 2)).transpose(1, 2)\n",
    "        #vecor in the from of sequence vector filled wtih 0\n",
    "        out = torch.zeros_like(seq_vector)\n",
    "        for i in range(self.block_size):\n",
    "            #loop over block element\n",
    "            for j in range(i):\n",
    "                #add to one element the feature vector of all previous elements but not the elemnts infron of it\n",
    "                out[:, i::self.block_size] += features[:, j::self.block_size]\n",
    "            #add bias i to every following block elemnt on pos i\n",
    "            out[:, i::self.block_size] += self.bias[i]\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To achieve the final generative output, we employ a linear head for the first three layers, which **don't involve compression**. Following that, we switch to the convolutional head for the next three layers to **reverse the compression**. This results in **autoregressive** and **layer-wise** token generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Generative Head module for transforming the output of the transformer into target value logits.\n",
    "\n",
    "    Args:\n",
    "        num_vocab (int): Number of vocabulary items.\n",
    "        embed_dim (int): Dimensionality of the embeddings.\n",
    "        head_dim (int): Dimensionality of the intermediate representation.\n",
    "        resolution (int): Resolution of the 3D grid.\n",
    "\n",
    "    Attributes:\n",
    "        num_vocab (int): Number of vocabulary items.\n",
    "        embed_dim (int): Dimensionality of the embeddings.\n",
    "        head_dim (int): Dimensionality of the intermediate representation.\n",
    "        resolution (int): Resolution of the 3D grid.\n",
    "        fc (nn.Linear): Linear layer for transformation.\n",
    "        heads (list): List of different heads for transformation.\n",
    "        reduction_factor (dict): Dictionary containing reduction factors for each layer depth.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_vocab, embed_dim, head_dim, resolution):\n",
    "        \"\"\"\n",
    "        Initialize the GenerativeHead module.\n",
    "\n",
    "        Args:\n",
    "            num_vocab (int): Number of vocabulary items.\n",
    "            embed_dim (int): Dimensionality of the embeddings.\n",
    "            head_dim (int): Dimensionality of the intermediate representation.\n",
    "            resolution (int): Resolution of the 3D grid.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.num_vocab = num_vocab\n",
    "        self.embed_dim = embed_dim\n",
    "        self.head_dim = head_dim\n",
    "        self.resolution = resolution\n",
    "        self.fc = nn.Linear(embed_dim, num_vocab)\n",
    "        self.heads = [\n",
    "            LinearHead(num_vocab, embed_dim, resolution),\n",
    "            LinearHead(num_vocab, embed_dim, resolution),\n",
    "            LinearHead(num_vocab, embed_dim, resolution),\n",
    "            ConvolutionalHead(num_vocab, embed_dim, head_dim, resolution, conv_size=4),\n",
    "            ConvolutionalHead(num_vocab, embed_dim, head_dim, resolution, conv_size=8),\n",
    "            ConvolutionalHead(num_vocab, embed_dim, head_dim, resolution, conv_size=8)\n",
    "        ]\n",
    "        self.reduction_factor = {\n",
    "            1: 1,\n",
    "            2: 1,\n",
    "            3: 1,\n",
    "            4: 4,\n",
    "            5: 8,\n",
    "            6: 8\n",
    "        }\n",
    "\n",
    "    def forward(self, x, value, depth, position):\n",
    "        \"\"\"\n",
    "        Transform the output of the transformer into target value logits.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): Output of the transformer, the latent vector [N, T, E].\n",
    "            value (torch.Tensor): Target value token sequence [N, T].\n",
    "            depth (torch.Tensor): Target depth token sequence [N, T].\n",
    "            position (torch.Tensor): Target position token sequence [N, T, A].\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Logits of target value sequence.\n",
    "        \"\"\"\n",
    "        out = []\n",
    "\n",
    "        # Process each sample individually (N=1), also squeeze redundant\n",
    "        for latent_vec, val, dep, pos in zip(x, value, depth, position):\n",
    "\n",
    "            logits = torch.tensor([])\n",
    "            vector_idx = 0\n",
    "\n",
    "            # Compute logits layerwise\n",
    "            for layer_idx, head in enumerate(self.heads):\n",
    "                layer_depth = layer_idx + 1\n",
    "                # Get value, depth, and position sequence of current layer\n",
    "                layer_val = val[dep == layer_depth]\n",
    "                layer_dep = dep[dep == layer_depth]\n",
    "                layer_pos = pos[dep == layer_depth]\n",
    "                # stop if no more tokens in current layer\n",
    "                if layer_pos.shape[0] == 0:\n",
    "                    break\n",
    "                # Compute number of vectors in latent vector of current layer\n",
    "                # because we might have X tokens but only X/red_factor many feature vectors due to reduction\n",
    "                num_vectors = torch.sum(dep == layer_depth) // self.reduction_factor[layer_depth]\n",
    "\n",
    "                # Filter latent vector of current layer\n",
    "                layer_vec = latent_vec[vector_idx:vector_idx + num_vectors]\n",
    "                # Compute layer logits\n",
    "                layer_logits = head(\n",
    "                    layer_vec.unsqueeze(0),\n",
    "                    layer_val.unsqueeze(0),\n",
    "                    layer_dep.unsqueeze(0),\n",
    "                    layer_pos.unsqueeze(0),\n",
    "                )[0]\n",
    "                logits = torch.cat([logits, layer_logits])\n",
    "\n",
    "                # Discard processed tokens\n",
    "                vector_idx += num_vectors\n",
    "            out += [logits]\n",
    "\n",
    "        # Pad embedding sequence\n",
    "        return pad_sequence(out, batch_first=True, padding_value=0.0)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Stack\n",
    "![Transfromer](https://tfwiki.net/mediawiki/images2/thumb/3/37/Optimusg1.jpg/350px-Optimusg1.jpg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this stage, we construct the transformer stack using the PyTorch implementation. We focus solely on the encoder component, as is commonly done. It's important to recognize that even though our model uses the encoder, it behaves like a decoder by autoregressively generating tokens. The transfromer enables us to generate features out of the sequence embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer module for sequence processing using self-attention mechanism.\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): Dimensionality of the embeddings.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        dropout (float): Dropout rate for attention layers.\n",
    "        num_layers (int): Number of transformer layers.\n",
    "\n",
    "    Attributes:\n",
    "        sos (nn.Parameter): Start of sequence token.\n",
    "        transformer (nn.TransformerEncoder): Transformer encoder stack.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout, num_layers):\n",
    "        \"\"\"\n",
    "        Initialize the Transformer module.\n",
    "\n",
    "        Args:\n",
    "            embed_dim (int): Dimensionality of the embeddings.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            dropout (float): Dropout rate for attention layers.\n",
    "            num_layers (int): Number of transformer layers.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.sos = nn.Parameter(torch.zeros(embed_dim))\n",
    "        nn.init.normal_(self.sos)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim,\n",
    "            nhead=num_heads,\n",
    "            dim_feedforward=4 * embed_dim,\n",
    "            dropout=dropout,\n",
    "            activation='gelu',\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=num_layers,\n",
    "            norm=nn.LayerNorm(embed_dim))\n",
    "\n",
    "    def forward(self, input_seq, padding_mask):\n",
    "        \"\"\"\n",
    "        Apply the Transformer to input sequence.\n",
    "\n",
    "        Args:\n",
    "            input_seq (torch.Tensor): Input sequence tensor.\n",
    "                Shape: (batch_size, sequence_length, embed_dim)\n",
    "            padding_mask (torch.Tensor): Padding mask for the input sequence.\n",
    "                Shape: (batch_size, sequence_length)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output sequence tensor after processing by the Transformer.\n",
    "        \"\"\"\n",
    "        # Create a tensor of shape batch_size x 1 x embed_dim filled with the SOS token\n",
    "        sos = self.sos.unsqueeze(0).unsqueeze(0).repeat(input_seq.shape[0], 1, 1)\n",
    "        batch_size, seq_len, _ = input_seq.shape\n",
    "        # Concatenate the SOS token with the input sequence (except the last token)\n",
    "        input_seq = torch.cat([sos, input_seq[:, :-1]], dim=1)\n",
    "        # Generate mask for the attention mechanism\n",
    "        mask = get_mask(seq_len)\n",
    "        # Process input sequence through the Transformer stack to get the output sequence\n",
    "        output_seq = self.transformer(\n",
    "            src=input_seq,\n",
    "            mask=mask,  # [L, L]\n",
    "            src_key_padding_mask=padding_mask,  # [N, L]\n",
    "        )\n",
    "        return output_seq\n",
    "\n",
    "def get_mask(seq_len):\n",
    "    \"\"\"\n",
    "    Creates a diagonal mask to prevent self-attention from looking ahead.\n",
    "\n",
    "    Args:\n",
    "        seq_len (int): Length of the sequence.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Diagonal mask tensor.\n",
    "    \"\"\"\n",
    "    # create a matrix of dimension seq_len x seq_len filled with -Inf\n",
    "    attn_mask = torch.full((seq_len, seq_len), -float(\"Inf\"))\n",
    "    # just keep the upper triangular part of the matrix and use this as mask\n",
    "    return torch.triu(attn_mask, diagonal=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OctreeTransformer in PyTorch Lightning\n",
    "<div>\n",
    "<img src=\"https://cdn-fgnbn.nitrocdn.com/CExiFXHDXAeTGrlIkvRSSLnZISOqDumi/assets/images/optimized/rev-bd09e47/wp-content/uploads/2022/08/wx_mining-lightning-strike_features-benefits_1200x630.png\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "With our three crucial building blocks in place, it's time to connect the dots and form the **OctreeTransformer**. To keep things neat and simple, we've fashioned the OctreeTransformer as a **PyTorch Lightning** module. Thereby, we don't have to worry about training loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OctreeTransformer_pl(pl.LightningModule):\n",
    "    \"\"\"\n",
    "    Lightning module implementing an OctreeTransformer for sequence-to-sequence tasks.\n",
    "\n",
    "    Args:\n",
    "        embed_dim (int): Dimensionality of the embeddings.\n",
    "        num_heads (int): Number of attention heads.\n",
    "        dropout (float): Dropout rate for attention layers.\n",
    "        num_layers (int): Number of transformer layers.\n",
    "        num_vocab (int): Number of vocabulary items, including the padding index.\n",
    "        resolution (int): Resolution of the 3D grid.\n",
    "\n",
    "    Attributes:\n",
    "        embedding (SequenceEmbedding): Sequence embedding module.\n",
    "        transformer (Transformer): Transformer module.\n",
    "        head (GenerativeHead): Generative head module.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, embed_dim, num_heads, dropout, num_layers, num_vocab, resolution):\n",
    "        \"\"\"\n",
    "        Initialize the OctreeTransformer_pl module.\n",
    "\n",
    "        Args:\n",
    "            embed_dim (int): Dimensionality of the embeddings.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            dropout (float): Dropout rate for attention layers.\n",
    "            num_layers (int): Number of transformer layers.\n",
    "            num_vocab (int): Number of vocabulary items, including the padding index.\n",
    "            resolution (int): Resolution of the 3D grid.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        print(\"Please note that num_vocab should include the padding index\")\n",
    "        self.embedding = SequenceEmbedding(num_vocab, embed_dim, resolution)\n",
    "        self.transformer = Transformer(embed_dim, num_heads, dropout, num_layers)\n",
    "        self.head = GenerativeHead(num_vocab, embed_dim, head_dim, resolution)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "    def forward(self, sequence):\n",
    "        \"\"\"\n",
    "        Forward pass of the OctreeTransformer_pl module.\n",
    "\n",
    "        Args:\n",
    "            sequence (tuple): Tuple containing (value, depth, position) sequences.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Output tensor after processing through the OctreeTransformer.\n",
    "        \"\"\"\n",
    "        value, depth, position = sequence\n",
    "        # create embedding\n",
    "        embeddings = self.embedding(value, depth, position)\n",
    "        # pass through transformer, with respective mask calculated from embedding\n",
    "        encoder_output = self.transformer(embeddings, self.embedding.get_mask())\n",
    "        # pass through head\n",
    "        x = self.head(encoder_output, value, depth, position)\n",
    "        return x\n",
    "\n",
    "    def step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Step function for training and evaluation steps.\n",
    "\n",
    "        Args:\n",
    "            batch (tuple): Tuple containing input sequence and target tensor.\n",
    "            batch_idx (int): Index of the batch.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Calculated loss value.\n",
    "        \"\"\"\n",
    "        sequence, target = batch\n",
    "        # get logits\n",
    "        output = self(sequence)\n",
    "        # calculate loss\n",
    "        loss = self.calculate_loss(output, target)\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # training step for pl module\n",
    "        loss = self.step(batch, batch_idx)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # validation step for pl module\n",
    "        loss = self.step(batch, batch_idx)\n",
    "        self.log('val_loss', loss)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        # test step for pl module\n",
    "        loss = self.step(batch, batch_idx)\n",
    "        self.log('test_loss', loss)\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        # conifgure optimizer and lr scheduler\n",
    "        # AdamW optimizer \n",
    "        optimizer = AdamW(self.parameters(), lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0.01)\n",
    "        # Cosine annealing warm restarts scheduler\n",
    "        lr_scheduler = CosineAnnealingWarmRestarts(optimizer, 200)\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def calculate_loss(self, output, target):\n",
    "        \"\"\"\n",
    "        Calculate the loss between the output and target sequences.\n",
    "\n",
    "        Args:\n",
    "            output (torch.Tensor): Output tensor from the model.\n",
    "            target (torch.Tensor): Target tensor containing true values.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: Calculated loss value.\n",
    "        \"\"\"\n",
    "        # cross entropy loss\n",
    "        loss_function = nn.CrossEntropyLoss(ignore_index=PADDING_VALUE)\n",
    "        # Change shape to batch size x class x sequence length for cross entropy loss\n",
    "        output = output.permute(0, 2, 1)\n",
    "        # calculate loss\n",
    "        return loss_function(output, target)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Now everything is set up...\n",
    "\n",
    "*Let's start the training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resolution: 64\n",
      "Embedding Dimension: 32\n",
      "Number of Heads: 1\n",
      "Dropout: 0.1\n",
      "Number of Layers: 2\n",
      "Please note that num_vocab should include the padding index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name        | Type              | Params\n",
      "--------------------------------------------------\n",
      "0 | embedding   | SequenceEmbedding | 0     \n",
      "1 | transformer | Transformer       | 25.5 K\n",
      "2 | head        | GenerativeHead    | 132   \n",
      "--------------------------------------------------\n",
      "25.6 K    Trainable params\n",
      "0         Non-trainable params\n",
      "25.6 K    Total params\n",
      "0.103     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a9bfbe2a9a49988c25457452b88990",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=250` reached.\n"
     ]
    }
   ],
   "source": [
    "tb_logger = pl_loggers.TensorBoardLogger('logs/')\n",
    "\n",
    "resolution = 64\n",
    "embed_dim = 32\n",
    "num_heads = 1\n",
    "dropout = 0.1\n",
    "num_layers = 2\n",
    "#print all hyperparameters\n",
    "print(f\"Resolution: {resolution}\")\n",
    "print(f\"Embedding Dimension: {embed_dim}\")\n",
    "print(f\"Number of Heads: {num_heads}\")\n",
    "print(f\"Dropout: {dropout}\")\n",
    "print(f\"Number of Layers: {num_layers}\")\n",
    "\n",
    "model = OctreeTransformer_pl(embed_dim, num_heads, dropout, num_layers, NUM_VOCAB, resolution)\n",
    "\n",
    "# Load the dataset\n",
    "dataset = ShapeNet(\"../data\")\n",
    "# Create data loaders\n",
    "train_loader = DataLoader(dataset, collate_fn=EncoderOnlyCollate(), batch_size=1, shuffle=True)\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = pl.Trainer(max_epochs=250, log_every_n_steps=1, logger=tb_logger)\n",
    "trainer.fit(model, train_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Shapes\n",
    "\n",
    "![shapes](../images/samples.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To generate shapes, we have to create a token generator. This generator samples tokens in an autoregressive way with respect to the kernel size which was used for compression. Thereby, it samples kernel size many tokens each time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator:\n",
    "    def __init__(self, model, num_tokens=1, **_):\n",
    "        \"\"\" Create token generator instance which samples 'num_tokens' in one pass.\n",
    "\n",
    "        Args:\n",
    "            model: OctreeTransformer model instance.\n",
    "            num_tokens: Defines the number of sampled tokens in each step.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.kernel_size = num_tokens\n",
    "\n",
    "    def __call__(self, val, dep, pos, temperature=1.0):\n",
    "        \"\"\" Sample autoregressive current value token sequence and return updated value sequence.\n",
    "\n",
    "        Args:\n",
    "            val: Value token sequence of current layer.\n",
    "            dep: Depth token sequence of current layer.\n",
    "            pos: Position token sequence of current layer.\n",
    "            temperature: Defines the randomness of the samples.\n",
    "\n",
    "        Return:\n",
    "            Sampled token sequence with values of the current layer.\n",
    "        \"\"\"\n",
    "        # compute indices\n",
    "        token_idx = 0\n",
    "        sampled_idx = len(torch.cat(val[:-1])) if len(val) > 1 else 0\n",
    "\n",
    "        # sample tokens autoregressive\n",
    "        for _ in trange(len(val[-1]) // self.kernel_size, leave=False, desc=\"Tokens\"):\n",
    "            for block_idx in range(self.kernel_size):\n",
    "                # concat layers and slice sequence for speed_up\n",
    "                seq = (\n",
    "                    torch.cat(val)[:sampled_idx + token_idx + self.kernel_size].unsqueeze(0),\n",
    "                    torch.cat(dep)[:sampled_idx + token_idx + self.kernel_size].unsqueeze(0),\n",
    "                    torch.cat(pos)[:sampled_idx + token_idx + self.kernel_size].unsqueeze(0),\n",
    "                )\n",
    "\n",
    "                logits = self.model(seq)[0]\n",
    "\n",
    "                # retrieve only logits for for current index\n",
    "                sampled_token_logits = logits[sampled_idx + token_idx + block_idx]\n",
    "\n",
    "                # compute token probabilities from logits\n",
    "                sampled_token_logits[0] = -float(\"Inf\")  # 'padding' token\n",
    "                print(sampled_token_logits)\n",
    "                probs = torch.nn.functional.softmax(sampled_token_logits / temperature, dim=-1)  # [t, V]\n",
    "                print(probs)\n",
    "                # sample next sequence token\n",
    "                val[-1][token_idx + block_idx] = torch.multinomial(probs, num_samples=1)[0]\n",
    "\n",
    "            # update indices\n",
    "            token_idx += self.kernel_size\n",
    "\n",
    "        return val[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we're introducing a Sampler class into the mix. For each layer, we have a Generator that predicts additional tokens by building upon the compression we performed earlier. When we use the sampler, it follows a specific sequence. Initially, it selects a **random voxel** using the precondition_resolution. Afterward, the sampler generates **additional layers** as needed to achieve the intended target_resolution. For each of these layers, there's a **dedicated generator** responsible for producing tokens based on predictions made by the model. In essence, the process involves progressively generating more details while moving towards the desired resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    def __init__(self, model, max_resolution, **_):\n",
    "        \"\"\" Provides a basic implementation of the sampler for the 'encoder_only' architecture.\n",
    "\n",
    "        Args:\n",
    "            model: Model which is used for sampling.\n",
    "            max_resolution: Maximum resolution the model is trained on.\n",
    "        \"\"\"\n",
    "        self.generators = [\n",
    "            Generator(model,1),\n",
    "            Generator(model,1),\n",
    "            Generator(model,1), \n",
    "            Generator(model,4), \n",
    "            Generator(model,8),\n",
    "            Generator(model,8), \n",
    "        ]\n",
    "\n",
    "        self.max_resolution = max_resolution\n",
    "\n",
    "    def __call__(self, precondition_resolution, target_resolution, temperature):\n",
    "        \"\"\" Perform an iterative sampling of the given sequence until reaching the end of sequence, the maximum sequence\n",
    "            length or the desired resolution.\n",
    "\n",
    "        Args:\n",
    "            precondition: An array of elements (pixels/voxels) as an numpy array.\n",
    "            precondition_resolution: Resolution at which the autoencoder will reconstruct the layer.\n",
    "            target_resolution: Resolution up to which an object should be sampled.\n",
    "            temperature: Defines the randomness of the samples.\n",
    "\n",
    "        Return:\n",
    "            A token sequence with values, encoding the final sample.\n",
    "        \"\"\"\n",
    "        #get sample\n",
    "        val, dep, pos = self.generate_sample(precondition_resolution)\n",
    "\n",
    "        # compute the number of finished (current) layers and the maximum sampleable layer\n",
    "        cur_layer = len(val)\n",
    "        max_layer = int(math.log2(min(target_resolution, self.max_resolution)))\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            # sample layer-wise\n",
    "            for idx in tqdm(range(cur_layer, max_layer), initial=cur_layer, total=max_layer, leave=True, desc=\"Layers\"):\n",
    "\n",
    "                # init sequences for next layer\n",
    "                next_val, next_dep, next_pos = next_layer_tokens(\n",
    "                    val, dep, pos, self.max_resolution\n",
    "                )\n",
    "                # predict value tokens for current layer\n",
    "                next_val = self.generators[idx](\n",
    "                    val=val + [next_val],\n",
    "                    dep=dep + [next_dep],\n",
    "                    pos=pos + [next_pos],\n",
    "                    temperature=temperature\n",
    "                )\n",
    "\n",
    "                # append sampled tokens to current sequence\n",
    "                val += [next_val]\n",
    "                dep += [next_dep]\n",
    "                pos += [next_pos]\n",
    "\n",
    "                if torch.sum(next_val == 2) == 0:\n",
    "                    break  # early-out, no mixed tokens sampled\n",
    "\n",
    "        return postprocess(val, target_resolution)\n",
    "    \n",
    "    def generate_sample(self, precondition_resolution):\n",
    "        \"\"\" Generate a random sample from the model.\n",
    "\n",
    "        Args:\n",
    "            precondition_resolution: Resolution at which the autoencoder will reconstruct the layer.\n",
    "\n",
    "        Return:\n",
    "            A token sequence with values, encoding the final sample.\n",
    "        \"\"\"\n",
    "\n",
    "        #generate random numpy array with values between 0-2\n",
    "        array_size = 3 * [self.max_resolution]\n",
    "        precondition = torch.randint(low=0, high=2, size=array_size, dtype=torch.long).numpy()\n",
    "\n",
    "        # convert input array into token sequence\n",
    "        tree = kdTree()\n",
    "        tree = tree.insert_element_array(precondition, max_depth=math.log2(precondition_resolution) + 1)\n",
    "        value, depth, position = tree.get_token_sequence(\n",
    "            depth=math.log2(precondition_resolution), return_depth=True, return_pos=True\n",
    "        )\n",
    "\n",
    "        val = []\n",
    "        dep = []\n",
    "        pos = []\n",
    "        # extract each depth layer separately and convert to PyTorch as a long tensor\n",
    "        for d in range(1, max(depth) + 1):\n",
    "            val += [torch.tensor(value[depth == d], dtype=torch.long)]\n",
    "            dep += [torch.tensor(depth[depth == d], dtype=torch.long)]\n",
    "            pos += [torch.tensor(position[depth == d], dtype=torch.long)]\n",
    "\n",
    "        return val, dep, pos\n",
    "    \n",
    "    def postprocess(self,value, target_resolution):\n",
    "        \"\"\" Transform sequence of value tokens into an array of elements (voxels/pixels).\n",
    "\n",
    "        Args:\n",
    "            value: List of value token sequences for each layer as pytorch tensors.\n",
    "            target_resolution: Resolution up to which an object should be sampled.\n",
    "\n",
    "        Return:\n",
    "            An array of elements as a numpy array.\n",
    "        \"\"\"\n",
    "        # concat all layers\n",
    "        value = torch.cat(value)\n",
    "\n",
    "        # move value sequence to the cpu and convert to numpy array\n",
    "        value = value.cpu().numpy()\n",
    "\n",
    "        # insert the sequence into a kd-tree\n",
    "        tree = kdTree().insert_token_sequence(\n",
    "            value,\n",
    "            resolution=target_resolution\n",
    "        )\n",
    "\n",
    "        # retrive pixels/voxels from the kd-tree\n",
    "        return tree.get_element_array(mode=\"occupancy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function generates the next layer of tokens with their positions and depth, used by the sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_layer_tokens(value, depth, position, max_resolution):\n",
    "    \"\"\" Creates artificial tokens for the next layer of the value sequence, to match the predefined shape. Precomputes\n",
    "    corresponding depth and position tokens of the sequence, too.\n",
    "\n",
    "    Args:\n",
    "        value: List of value token sequences for each layer as pytorch tensors.\n",
    "        depth: List of depth token sequences for each layer as pytorch tensors.\n",
    "        position: List of position token sequences for each layer as pytorch tensors.\n",
    "        max_resolution: The maximal resolution the corresponding model is trained for.\n",
    "\n",
    "    Return:\n",
    "        Pre-initialised next layer sequence (value, depth, position).\n",
    "    \"\"\"\n",
    "    dirs = np.array(list(itertools.product([1, 2], repeat=3)))\n",
    "    num_children = 2**SPATIAL_DIM\n",
    "\n",
    "    # got an empty input - initialize with default values and return\n",
    "    if len(value[0]) == 0:\n",
    "        value = torch.tensor(num_children * [1], dtype=torch.long)\n",
    "        depth = torch.tensor(num_children * [1], dtype=torch.long)\n",
    "        pos = (\n",
    "            torch.ones(num_children, SPATIAL_DIM, dtype=torch.long) *\n",
    "            torch.tensor(dirs)\n",
    "        )\n",
    "    # compute next layer depth and number of future tokens\n",
    "    cur_depth = len(value)\n",
    "    num_future_tokens = num_children * torch.sum(value[-1] == 2)\n",
    "\n",
    "    # compute future sequence (non padding token) and future depth sequence\n",
    "    nl_value = torch.tensor([1], dtype=torch.long).repeat(num_future_tokens)\n",
    "    nl_depth = torch.tensor([cur_depth + 1], dtype=torch.long).repeat(num_future_tokens)\n",
    "\n",
    "    # retrive and copy mixed tokens positions\n",
    "    pos_token = position[-1][value[-1] == 2]\n",
    "    nl_pos = torch.repeat_interleave(pos_token, num_children, dim=0)\n",
    "\n",
    "    # compute position difference and add it to future positions with respect to predefined pattern\n",
    "\n",
    "    nl_pos = 2 * nl_pos + torch.tensor(dirs).repeat(pos_token.shape[0], 1)\n",
    "    return nl_value, nl_depth, nl_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create a Sampler instance and sample a voxel. *Let's see what our model has learned*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "please note that num_vocab should include the padding index\n",
      "lin: 256->4\n",
      "lin: 256->4\n",
      "lin: 256->4\n",
      "Deconv: 256 -> 16\n",
      "BlockConv: 16 -> 16\n",
      "Embed: 4 -> 16\n",
      "Linear: 16 -> 4\n",
      "Deconv: 256 -> 16\n",
      "BlockConv: 16 -> 16\n",
      "Embed: 4 -> 16\n",
      "Linear: 16 -> 4\n",
      "Deconv: 256 -> 16\n",
      "BlockConv: 16 -> 16\n",
      "Embed: 4 -> 16\n",
      "Linear: 16 -> 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08bbb4221bca43bfb9ea950ca9a78eca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokens:   0%|          | 0/64 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   -inf,  1.3849, -2.5565,  0.4849])\n",
      "tensor([0.0000, 0.7013, 0.0136, 0.2851])\n",
      "tensor([   -inf,  2.9339,  0.1103, -0.3737])\n",
      "tensor([0.0000, 0.9124, 0.0542, 0.0334])\n",
      "tensor([   -inf, -0.4820, -3.2748, -1.2865])\n",
      "tensor([0.0000, 0.6629, 0.0406, 0.2965])\n",
      "tensor([   -inf,  1.0719, -0.6091, -2.1345])\n",
      "tensor([0.0000, 0.8152, 0.1518, 0.0330])\n",
      "tensor([   -inf,  2.8254, -1.3615,  1.6473])\n",
      "tensor([0.0000, 0.7558, 0.0115, 0.2327])\n",
      "tensor([  -inf, 4.3874, 1.2967, 0.8104])\n",
      "tensor([0.0000, 0.9316, 0.0424, 0.0260])\n",
      "tensor([   -inf,  0.9711, -2.0853, -0.1083])\n",
      "tensor([0.0000, 0.7210, 0.0339, 0.2450])\n",
      "tensor([   -inf,  2.5272,  0.5776, -0.9523])\n",
      "tensor([0.0000, 0.8524, 0.1213, 0.0263])\n",
      "tensor([   -inf, -0.4299, -1.6497, -0.3686])\n",
      "tensor([0.0000, 0.4240, 0.1252, 0.4508])\n",
      "tensor([   -inf,  1.1227, -0.1564,  0.0683])\n",
      "tensor([0.0000, 0.6147, 0.1711, 0.2142])\n",
      "tensor([   -inf, -2.2974, -2.3668, -2.1415])\n",
      "tensor([0.0000, 0.3224, 0.3008, 0.3768])\n",
      "tensor([   -inf, -0.7354, -0.8777, -1.6896])\n",
      "tensor([0.0000, 0.4440, 0.3851, 0.1710])\n",
      "tensor([   -inf,  0.9930, -0.4483,  0.7701])\n",
      "tensor([0.0000, 0.4909, 0.1162, 0.3929])\n",
      "tensor([  -inf, 2.5615, 1.0343, 1.2312])\n",
      "tensor([0.0000, 0.6750, 0.1466, 0.1785])\n",
      "tensor([   -inf, -0.8583, -1.1754, -0.9841])\n",
      "tensor([0.0000, 0.3831, 0.2790, 0.3378])\n",
      "tensor([   -inf,  0.7082,  0.3127, -0.5221])\n",
      "tensor([0.0000, 0.5088, 0.3426, 0.1487])\n",
      "tensor([   -inf,  1.8156, -3.2098,  0.2645])\n",
      "tensor([0.0000, 0.8206, 0.0054, 0.1740])\n",
      "tensor([   -inf,  3.3377, -0.5324, -0.6304])\n",
      "tensor([0.0000, 0.9618, 0.0201, 0.0182])\n",
      "tensor([   -inf,  0.6235, -2.6702,  0.6540])\n",
      "tensor([0.0000, 0.4836, 0.0179, 0.4985])\n",
      "tensor([   -inf,  2.1768, -0.0052, -0.1982])\n",
      "tensor([0.0000, 0.8293, 0.0936, 0.0771])\n",
      "tensor([   -inf,  3.2639, -2.0194,  1.4399])\n",
      "tensor([0.0000, 0.8573, 0.0044, 0.1384])\n",
      "tensor([  -inf, 4.7847, 0.6557, 0.5444])\n",
      "tensor([0.0000, 0.9704, 0.0156, 0.0140])\n",
      "tensor([   -inf,  2.0663, -1.4758,  1.8204])\n",
      "tensor([0.0000, 0.5522, 0.0160, 0.4318])\n",
      "tensor([  -inf, 3.6243, 1.1831, 0.9744])\n",
      "tensor([0.0000, 0.8638, 0.0752, 0.0610])\n",
      "tensor([   -inf, -0.0202, -2.2924, -0.6175])\n",
      "tensor([0.0000, 0.6048, 0.0623, 0.3328])\n",
      "tensor([   -inf,  1.5320, -0.8022, -0.1799])\n",
      "tensor([0.0000, 0.7828, 0.0758, 0.1413])\n",
      "tensor([   -inf, -1.1916, -1.7634, -0.2020])\n",
      "tensor([0.0000, 0.2350, 0.1327, 0.6323])\n",
      "tensor([   -inf,  0.3702, -0.2737,  0.2514])\n",
      "tensor([0.0000, 0.4144, 0.2177, 0.3680])\n",
      "tensor([   -inf,  1.4236, -1.1032,  0.5467])\n",
      "tensor([0.0000, 0.6684, 0.0534, 0.2781])\n",
      "tensor([  -inf, 2.9800, 0.3859, 0.9936])\n",
      "tensor([0.0000, 0.8252, 0.0616, 0.1132])\n",
      "tensor([   -inf,  0.2362, -0.5668,  0.9418])\n",
      "tensor([0.0000, 0.2879, 0.1290, 0.5831])\n",
      "tensor([  -inf, 1.8227, 0.9103, 1.4310])\n",
      "tensor([0.0000, 0.4813, 0.1933, 0.3253])\n",
      "tensor([   -inf,  1.0651, -1.7574, -1.2478])\n",
      "tensor([0.0000, 0.8632, 0.0513, 0.0854])\n",
      "tensor([   -inf,  2.5425,  0.9392, -2.2073])\n",
      "tensor([0.0000, 0.8265, 0.1663, 0.0072])\n",
      "tensor([   -inf, -0.8550, -2.4511, -3.0943])\n",
      "tensor([0.0000, 0.7638, 0.1548, 0.0814])\n",
      "tensor([   -inf,  0.6920,  0.2137, -3.9549])\n",
      "tensor([0.0000, 0.6137, 0.3804, 0.0059])\n",
      "tensor([   -inf,  1.2398, -0.7646,  0.9173])\n",
      "tensor([0.0000, 0.5379, 0.0725, 0.3896])\n",
      "tensor([  -inf, 2.7567, 1.9126, 0.0138])\n",
      "tensor([0.0000, 0.6692, 0.2877, 0.0431])\n",
      "tensor([   -inf, -0.6420, -1.4767, -0.8766])\n",
      "tensor([0.0000, 0.4495, 0.1951, 0.3555])\n",
      "tensor([   -inf,  0.9083,  1.1899, -1.7322])\n",
      "tensor([0.0000, 0.4173, 0.5530, 0.0298])\n",
      "tensor([   -inf, -0.8500, -0.8081, -2.2484])\n",
      "tensor([0.0000, 0.4367, 0.4554, 0.1079])\n",
      "tensor([   -inf,  0.7411,  0.6665, -1.7495])\n",
      "tensor([0.0000, 0.4973, 0.4615, 0.0412])\n",
      "tensor([   -inf, -2.6823, -1.5415, -3.9699])\n",
      "tensor([0.0000, 0.2270, 0.7104, 0.0626])\n",
      "tensor([   -inf, -1.1191, -0.0531, -3.5122])\n",
      "tensor([0.0000, 0.2503, 0.7268, 0.0229])\n",
      "tensor([   -inf, -0.5950,  0.1493,  0.0368])\n",
      "tensor([0.0000, 0.2006, 0.4222, 0.3773])\n",
      "tensor([  -inf, 0.9447, 1.6462, 0.4567])\n",
      "tensor([0.0000, 0.2754, 0.5555, 0.1691])\n",
      "tensor([   -inf, -2.4876, -0.5603, -1.7747])\n",
      "tensor([0.0000, 0.1009, 0.6933, 0.2058])\n",
      "tensor([   -inf, -0.9250,  0.9310, -1.3200])\n",
      "tensor([0.0000, 0.1239, 0.7927, 0.0835])\n",
      "tensor([   -inf,  1.3980, -2.3698, -1.6096])\n",
      "tensor([0.0000, 0.9324, 0.0215, 0.0461])\n",
      "tensor([   -inf,  2.9528,  0.2924, -2.4561])\n",
      "tensor([0.0000, 0.9308, 0.0651, 0.0042])\n",
      "tensor([   -inf,  0.2495, -1.8500, -1.1563])\n",
      "tensor([0.0000, 0.7312, 0.0896, 0.1793])\n",
      "tensor([   -inf,  1.8051,  0.8137, -2.0053])\n",
      "tensor([0.0000, 0.7178, 0.2663, 0.0159])\n",
      "tensor([   -inf,  1.6557, -1.4134,  0.6787])\n",
      "tensor([0.0000, 0.7028, 0.0327, 0.2645])\n",
      "tensor([   -inf,  3.1728,  1.2652, -0.2268])\n",
      "tensor([0.0000, 0.8461, 0.1256, 0.0282])\n",
      "tensor([   -inf,  0.4607, -0.8723,  1.0606])\n",
      "tensor([0.0000, 0.3241, 0.0855, 0.5905])\n",
      "tensor([  -inf, 2.0293, 1.7866, 0.2299])\n",
      "tensor([0.0000, 0.5128, 0.4023, 0.0848])\n",
      "tensor([   -inf, -0.4147, -1.4645, -2.4602])\n",
      "tensor([0.0000, 0.6760, 0.2366, 0.0874])\n",
      "tensor([   -inf,  1.1462,  0.0230, -2.0067])\n",
      "tensor([0.0000, 0.7310, 0.2378, 0.0312])\n",
      "tensor([   -inf, -1.5885, -0.9335, -2.0463])\n",
      "tensor([0.0000, 0.2811, 0.5411, 0.1778])\n",
      "tensor([   -inf, -0.0054,  0.5453, -1.5615])\n",
      "tensor([0.0000, 0.3395, 0.5889, 0.0716])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   -inf, -0.1669, -0.5051, -0.1880])\n",
      "tensor([0.0000, 0.3714, 0.2649, 0.3637])\n",
      "tensor([  -inf, 1.3391, 1.0051, 0.1858])\n",
      "tensor([0.0000, 0.4922, 0.3524, 0.1553])\n",
      "tensor([   -inf, -1.3746,  0.0406,  0.1740])\n",
      "tensor([0.0000, 0.1018, 0.4192, 0.4790])\n",
      "tensor([  -inf, 0.1994, 1.5261, 0.6449])\n",
      "tensor([0.0000, 0.1580, 0.5954, 0.2467])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7be5561c8412449c81130f21323aaa7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokens:   0%|          | 0/112 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   -inf,  3.0055, -0.9088,  2.4607])\n",
      "tensor([0.0000, 0.6250, 0.0125, 0.3625])\n",
      "tensor([  -inf, 3.9208, 0.6996, 3.3033])\n",
      "tensor([0.0000, 0.6332, 0.0253, 0.3415])\n",
      "tensor([   -inf,  1.0005, -1.6800,  2.0043])\n",
      "tensor([0.0000, 0.2634, 0.0180, 0.7186])\n",
      "tensor([   -inf,  1.9315, -0.0464,  2.8710])\n",
      "tensor([0.0000, 0.2705, 0.0374, 0.6921])\n",
      "tensor([   -inf,  2.1399, -0.2561,  0.2557])\n",
      "tensor([0.0000, 0.8045, 0.0733, 0.1222])\n",
      "tensor([  -inf, 3.0810, 1.4321, 1.1805])\n",
      "tensor([0.0000, 0.7453, 0.1433, 0.1114])\n",
      "tensor([   -inf,  0.1693, -0.9344, -0.1067])\n",
      "tensor([0.0000, 0.4784, 0.1586, 0.3630])\n",
      "tensor([  -inf, 1.1028, 0.7128, 0.7765])\n",
      "tensor([0.0000, 0.4169, 0.2823, 0.3008])\n",
      "tensor([   -inf, -0.1696, -0.9833, -2.1662])\n",
      "tensor([0.0000, 0.6333, 0.2807, 0.0860])\n",
      "tensor([   -inf, -1.9076,  0.4594, -0.8191])\n",
      "tensor([0.0000, 0.0683, 0.7288, 0.2029])\n",
      "tensor([   -inf,  0.7060, -2.2013, -2.9563])\n",
      "tensor([0.0000, 0.9257, 0.0506, 0.0238])\n",
      "tensor([   -inf, -1.0288, -0.7512, -1.6023])\n",
      "tensor([0.0000, 0.3468, 0.4578, 0.1954])\n",
      "tensor([   -inf,  1.1314, -2.0067, -1.9212])\n",
      "tensor([0.0000, 0.9169, 0.0398, 0.0433])\n",
      "tensor([   -inf, -0.6012, -0.5412, -0.5480])\n",
      "tensor([0.0000, 0.3209, 0.3407, 0.3384])\n",
      "tensor([   -inf,  2.0055, -3.2169, -2.7024])\n",
      "tensor([0.0000, 0.9858, 0.0053, 0.0089])\n",
      "tensor([   -inf,  0.2771, -1.7495, -1.3305])\n",
      "tensor([0.0000, 0.7507, 0.0989, 0.1504])\n",
      "tensor([   -inf, -0.6687, -0.3926,  0.1604])\n",
      "tensor([0.0000, 0.2170, 0.2859, 0.4971])\n",
      "tensor([  -inf, 0.2665, 1.2683, 1.0610])\n",
      "tensor([0.0000, 0.1685, 0.4587, 0.3728])\n",
      "tensor([   -inf,  0.2080, -1.6006, -0.6179])\n",
      "tensor([0.0000, 0.6243, 0.1023, 0.2734])\n",
      "tensor([  -inf, 1.1384, 0.0412, 0.2532])\n",
      "tensor([0.0000, 0.5726, 0.1911, 0.2363])\n",
      "tensor([   -inf,  0.6300, -1.4219,  0.4005])\n",
      "tensor([0.0000, 0.5199, 0.0668, 0.4133])\n",
      "tensor([  -inf, 1.5718, 0.2651, 1.3273])\n",
      "tensor([0.0000, 0.4869, 0.1318, 0.3813])\n",
      "tensor([   -inf,  1.5111, -2.6105, -0.3596])\n",
      "tensor([0.0000, 0.8545, 0.0139, 0.1316])\n",
      "tensor([   -inf,  2.4426, -0.9672,  0.5160])\n",
      "tensor([0.0000, 0.8484, 0.0280, 0.1236])\n",
      "tensor([   -inf,  0.2880, -2.3131, -1.1487])\n",
      "tensor([0.0000, 0.7623, 0.0566, 0.1812])\n",
      "tensor([   -inf, -1.4552, -0.8832,  0.1849])\n",
      "tensor([0.0000, 0.1261, 0.2235, 0.6504])\n",
      "tensor([   -inf,  0.5734, -2.8448, -2.0769])\n",
      "tensor([0.0000, 0.9063, 0.0297, 0.0640])\n",
      "tensor([   -inf, -1.1486, -1.3648, -0.6910])\n",
      "tensor([0.0000, 0.2954, 0.2379, 0.4667])\n",
      "tensor([   -inf,  0.8033, -4.3320, -1.1044])\n",
      "tensor([0.0000, 0.8663, 0.0051, 0.1286])\n",
      "tensor([   -inf, -0.9306, -2.8754,  0.2554])\n",
      "tensor([0.0000, 0.2264, 0.0324, 0.7412])\n",
      "tensor([   -inf,  1.1010, -4.8351, -2.0031])\n",
      "tensor([0.0000, 0.9546, 0.0025, 0.0428])\n",
      "tensor([   -inf, -0.6418, -3.4014, -0.6647])\n",
      "tensor([0.0000, 0.4900, 0.0310, 0.4789])\n",
      "tensor([   -inf,  2.6176, -1.0709, -1.0359])\n",
      "tensor([0.0000, 0.9516, 0.0238, 0.0246])\n",
      "tensor([  -inf, 0.8862, 0.3863, 0.3302])\n",
      "tensor([0.0000, 0.4587, 0.2782, 0.2631])\n",
      "tensor([   -inf,  0.6487, -1.7583, -1.3985])\n",
      "tensor([0.0000, 0.8202, 0.0739, 0.1059])\n",
      "tensor([   -inf, -1.1064, -0.3623, -0.1068])\n",
      "tensor([0.0000, 0.1718, 0.3615, 0.4667])\n",
      "tensor([   -inf,  2.1194, -0.1427, -2.6746])\n",
      "tensor([0.0000, 0.8989, 0.0936, 0.0074])\n",
      "tensor([   -inf,  0.3763,  1.3003, -1.3281])\n",
      "tensor([0.0000, 0.2702, 0.6807, 0.0491])\n",
      "tensor([   -inf,  0.1295, -0.8699, -3.0838])\n",
      "tensor([0.0000, 0.7101, 0.2614, 0.0286])\n",
      "tensor([   -inf, -1.6114,  0.5602, -1.7538])\n",
      "tensor([0.0000, 0.0940, 0.8245, 0.0815])\n",
      "tensor([   -inf, -0.4856, -2.9288, -0.4080])\n",
      "tensor([0.0000, 0.4613, 0.0401, 0.4986])\n",
      "tensor([  -inf, 0.5337, 0.7374, 0.3927])\n",
      "tensor([0.0000, 0.3232, 0.3962, 0.2807])\n",
      "tensor([   -inf, -0.1863, -3.4305, -1.3024])\n",
      "tensor([0.0000, 0.7318, 0.0285, 0.2397])\n",
      "tensor([   -inf,  0.8387,  0.2564, -0.4810])\n",
      "tensor([0.0000, 0.5477, 0.3059, 0.1463])\n",
      "tensor([   -inf,  0.0555, -4.8891, -0.2944])\n",
      "tensor([0.0000, 0.5841, 0.0042, 0.4117])\n",
      "tensor([   -inf,  1.0575, -1.2574,  0.4632])\n",
      "tensor([0.0000, 0.6058, 0.0598, 0.3344])\n",
      "tensor([   -inf,  0.3380, -5.4217, -1.2283])\n",
      "tensor([0.0000, 0.8251, 0.0026, 0.1723])\n",
      "tensor([   -inf,  1.3630, -1.7363, -0.4095])\n",
      "tensor([0.0000, 0.8230, 0.0371, 0.1398])\n",
      "tensor([   -inf,  1.3750, -0.4058,  2.4589])\n",
      "tensor([0.0000, 0.2424, 0.0409, 0.7167])\n",
      "tensor([  -inf, 2.3056, 1.2360, 3.3402])\n",
      "tensor([0.0000, 0.2405, 0.0825, 0.6769])\n",
      "tensor([   -inf, -0.6250, -1.1735,  2.0080])\n",
      "tensor([0.0000, 0.0645, 0.0373, 0.8982])\n",
      "tensor([  -inf, 0.3144, 0.4873, 2.9034])\n",
      "tensor([0.0000, 0.0645, 0.0767, 0.8588])\n",
      "tensor([   -inf,  1.8945, -2.4232,  2.5069])\n",
      "tensor([0.0000, 0.3499, 0.0047, 0.6455])\n",
      "tensor([   -inf,  2.8380, -0.7340,  3.4319])\n",
      "tensor([0.0000, 0.3522, 0.0099, 0.6379])\n",
      "tensor([   -inf, -0.0999, -3.1653,  2.0795])\n",
      "tensor([0.0000, 0.1011, 0.0047, 0.8941])\n",
      "tensor([   -inf,  0.8462, -1.4893,  2.9929])\n",
      "tensor([0.0000, 0.1036, 0.0100, 0.8864])\n",
      "tensor([   -inf,  0.2581, -3.0042, -1.5721])\n",
      "tensor([0.0000, 0.8342, 0.0320, 0.1338])\n",
      "tensor([   -inf,  1.2863,  0.6921, -0.7395])\n",
      "tensor([0.0000, 0.5939, 0.3278, 0.0783])\n",
      "tensor([   -inf,  0.5748, -3.4563, -2.4127])\n",
      "tensor([0.0000, 0.9362, 0.0166, 0.0472])\n",
      "tensor([   -inf,  1.5915,  0.2065, -1.6172])\n",
      "tensor([0.0000, 0.7748, 0.1939, 0.0313])\n",
      "tensor([   -inf, -0.2186, -2.0064, -3.1347])\n",
      "tensor([0.0000, 0.8187, 0.1370, 0.0443])\n",
      "tensor([   -inf,  0.7847,  1.6200, -2.3825])\n",
      "tensor([0.0000, 0.2987, 0.6887, 0.0126])\n",
      "tensor([   -inf,  0.0621, -2.5473, -4.0752])\n",
      "tensor([0.0000, 0.9178, 0.0675, 0.0147])\n",
      "tensor([   -inf,  1.0881,  1.1314, -3.2633])\n",
      "tensor([0.0000, 0.4861, 0.5076, 0.0063])\n",
      "tensor([   -inf,  1.8501, -1.6701, -0.2723])\n",
      "tensor([0.0000, 0.8701, 0.0257, 0.1042])\n",
      "tensor([  -inf, 2.8773, 2.0178, 0.5536])\n",
      "tensor([0.0000, 0.6574, 0.2783, 0.0644])\n",
      "tensor([   -inf, -0.1204, -2.3635, -0.6419])\n",
      "tensor([0.0000, 0.5883, 0.0624, 0.3492])\n",
      "tensor([  -inf, 0.8919, 1.2831, 0.1330])\n",
      "tensor([0.0000, 0.3393, 0.5018, 0.1589])\n",
      "tensor([   -inf,  1.3583, -0.7223, -1.8923])\n",
      "tensor([0.0000, 0.8594, 0.1073, 0.0333])\n",
      "tensor([   -inf,  2.3729,  2.9471, -1.0918])\n",
      "tensor([0.0000, 0.3563, 0.6326, 0.0111])\n",
      "tensor([   -inf, -0.6336, -1.4570, -2.3082])\n",
      "tensor([0.0000, 0.6149, 0.2699, 0.1152])\n",
      "tensor([   -inf,  0.3791,  2.1836, -1.5406])\n",
      "tensor([0.0000, 0.1384, 0.8413, 0.0203])\n",
      "tensor([   -inf,  2.1193, -0.4851,  1.2910])\n",
      "tensor([0.0000, 0.6619, 0.0489, 0.2891])\n",
      "tensor([  -inf, 3.0639, 1.2033, 2.2201])\n",
      "tensor([0.0000, 0.6306, 0.0981, 0.2712])\n",
      "tensor([   -inf,  0.1413, -1.1847,  0.9102])\n",
      "tensor([0.0000, 0.2921, 0.0776, 0.6303])\n",
      "tensor([  -inf, 1.0638, 0.4271, 1.7568])\n",
      "tensor([0.0000, 0.2834, 0.1499, 0.5667])\n",
      "tensor([   -inf,  1.6187,  0.4468, -0.3469])\n",
      "tensor([0.0000, 0.6897, 0.2136, 0.0966])\n",
      "tensor([  -inf, 2.5564, 2.1210, 0.5623])\n",
      "tensor([0.0000, 0.5608, 0.3629, 0.0763])\n",
      "tensor([   -inf, -0.3753, -0.2925, -0.7678])\n",
      "tensor([0.0000, 0.3621, 0.3934, 0.2445])\n",
      "tensor([  -inf, 0.5673, 1.3766, 0.1326])\n",
      "tensor([0.0000, 0.2568, 0.5769, 0.1663])\n",
      "tensor([   -inf,  2.9332, -3.0546,  0.8717])\n",
      "tensor([0.0000, 0.8851, 0.0022, 0.1126])\n",
      "tensor([  -inf, 3.9698, 0.6541, 1.7141])\n",
      "tensor([0.0000, 0.8763, 0.0318, 0.0918])\n",
      "tensor([   -inf,  0.0928, -1.0906,  0.9480])\n",
      "tensor([0.0000, 0.2734, 0.0837, 0.6429])\n",
      "tensor([  -inf, 1.1108, 2.5796, 1.7508])\n",
      "tensor([0.0000, 0.1381, 0.6000, 0.2619])\n",
      "tensor([   -inf,  3.4823, -4.9925,  1.0054])\n",
      "tensor([0.0000e+00, 9.2232e-01, 1.9247e-04, 7.7484e-02])\n",
      "tensor([   -inf,  4.4958, -1.3330,  1.7934])\n",
      "tensor([0.0000, 0.9346, 0.0027, 0.0627])\n",
      "tensor([   -inf,  0.6283, -3.0549,  1.0498])\n",
      "tensor([0.0000, 0.3922, 0.0099, 0.5979])\n",
      "tensor([  -inf, 1.6409, 0.6016, 1.8381])\n",
      "tensor([0.0000, 0.3888, 0.1375, 0.4736])\n",
      "tensor([   -inf, -1.4563, -1.8892,  1.8324])\n",
      "tensor([0.0000, 0.0351, 0.0228, 0.9421])\n",
      "tensor([   -inf, -0.5411, -0.2881,  2.6704])\n",
      "tensor([0.0000, 0.0369, 0.0475, 0.9156])\n",
      "tensor([   -inf, -0.5961, -3.1516,  0.9969])\n",
      "tensor([0.0000, 0.1668, 0.0130, 0.8203])\n",
      "tensor([   -inf,  0.3381, -1.4942,  1.8877])\n",
      "tensor([0.0000, 0.1704, 0.0273, 0.8024])\n",
      "tensor([   -inf, -0.9427, -3.9305,  1.8558])\n",
      "tensor([0.0000, 0.0572, 0.0029, 0.9399])\n",
      "tensor([       -inf, -1.4238e-05, -2.2388e+00,  2.7841e+00])\n",
      "tensor([0.0000, 0.0578, 0.0062, 0.9360])\n",
      "tensor([   -inf, -0.0732, -5.1456,  1.0662])\n",
      "tensor([0.0000, 0.2421, 0.0015, 0.7564])\n",
      "tensor([   -inf,  0.8599, -3.4977,  1.9472])\n",
      "tensor([0.0000, 0.2513, 0.0032, 0.7455])\n",
      "tensor([   -inf,  3.6795, -3.1202, -0.2857])\n",
      "tensor([0.0000, 0.9803, 0.0011, 0.0186])\n",
      "tensor([  -inf, 4.7303, 0.6284, 0.6053])\n",
      "tensor([0.0000, 0.9683, 0.0160, 0.0157])\n",
      "tensor([   -inf,  0.8540, -1.1152, -0.1593])\n",
      "tensor([0.0000, 0.6655, 0.0929, 0.2416])\n",
      "tensor([  -inf, 1.8732, 2.5602, 0.6457])\n",
      "tensor([0.0000, 0.3048, 0.6059, 0.0893])\n",
      "tensor([   -inf,  3.2036, -2.1212, -1.8492])\n",
      "tensor([0.0000, 0.9889, 0.0048, 0.0063])\n",
      "tensor([   -inf,  4.2302,  1.5626, -1.0315])\n",
      "tensor([0.0000, 0.9306, 0.0646, 0.0048])\n",
      "tensor([   -inf,  0.3527, -0.1849, -1.8002])\n",
      "tensor([0.0000, 0.5882, 0.3435, 0.0683])\n",
      "tensor([   -inf,  1.3667,  3.4760, -1.0133])\n",
      "tensor([0.0000, 0.1071, 0.8830, 0.0099])\n",
      "tensor([   -inf, -0.7158, -1.9731,  0.6578])\n",
      "tensor([0.0000, 0.1911, 0.0543, 0.7546])\n",
      "tensor([   -inf,  0.2224, -0.3156,  1.5571])\n",
      "tensor([0.0000, 0.1858, 0.1085, 0.7057])\n",
      "tensor([   -inf,  0.1683, -3.1717, -0.1091])\n",
      "tensor([0.0000, 0.5577, 0.0198, 0.4226])\n",
      "tensor([   -inf,  1.1002, -1.5255,  0.7674])\n",
      "tensor([0.0000, 0.5589, 0.0405, 0.4007])\n",
      "tensor([   -inf, -1.2162, -1.0483, -0.9880])\n",
      "tensor([0.0000, 0.2908, 0.3439, 0.3653])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   -inf, -0.2746,  0.6339, -0.0636])\n",
      "tensor([0.0000, 0.2121, 0.5260, 0.2619])\n",
      "tensor([   -inf, -0.3419, -2.2578, -1.7666])\n",
      "tensor([0.0000, 0.7206, 0.1061, 0.1734])\n",
      "tensor([   -inf,  0.5986, -0.5906, -0.8659])\n",
      "tensor([0.0000, 0.6512, 0.1983, 0.1505])\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4477640422143faa5f2ea297d3bd9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokens:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   -inf, -0.2659,  0.1084,  0.3201])\n",
      "tensor([0.0000, 0.2353, 0.3420, 0.4227])\n",
      "tensor([   -inf, -0.2549,  0.0963,  0.2864])\n",
      "tensor([0.0000, 0.2416, 0.3433, 0.4151])\n",
      "tensor([   -inf, -0.2722,  0.0772,  0.2953])\n",
      "tensor([0.0000, 0.2391, 0.3391, 0.4218])\n",
      "tensor([   -inf, -0.1782,  0.0384,  0.1963])\n",
      "tensor([0.0000, 0.2706, 0.3360, 0.3935])\n",
      "tensor([   -inf, -0.2588,  0.1088,  0.3199])\n",
      "tensor([0.0000, 0.2365, 0.3416, 0.4219])\n",
      "tensor([   -inf, -0.2567,  0.0928,  0.2974])\n",
      "tensor([0.0000, 0.2404, 0.3411, 0.4185])\n",
      "tensor([   -inf, -0.2739,  0.0807,  0.2958])\n",
      "tensor([0.0000, 0.2385, 0.3400, 0.4216])\n",
      "tensor([   -inf, -0.1821,  0.0306,  0.1873])\n",
      "tensor([0.0000, 0.2714, 0.3358, 0.3928])\n",
      "tensor([   -inf, -0.2543,  0.1094,  0.3220])\n",
      "tensor([0.0000, 0.2371, 0.3411, 0.4218])\n",
      "tensor([   -inf, -0.2551,  0.0940,  0.2928])\n",
      "tensor([0.0000, 0.2411, 0.3418, 0.4170])\n",
      "tensor([   -inf, -0.2720,  0.0890,  0.2979])\n",
      "tensor([0.0000, 0.2379, 0.3414, 0.4207])\n",
      "tensor([   -inf, -0.1744,  0.0416,  0.1928])\n",
      "tensor([0.0000, 0.2714, 0.3368, 0.3918])\n",
      "tensor([   -inf, -0.2341,  0.1033,  0.3266])\n",
      "tensor([0.0000, 0.2408, 0.3374, 0.4218])\n",
      "tensor([   -inf, -0.2595,  0.0924,  0.2956])\n",
      "tensor([0.0000, 0.2402, 0.3414, 0.4184])\n",
      "tensor([   -inf, -0.2796,  0.0487,  0.2864])\n",
      "tensor([0.0000, 0.2410, 0.3346, 0.4244])\n",
      "tensor([   -inf, -0.1854,  0.0275,  0.1897])\n",
      "tensor([0.0000, 0.2708, 0.3351, 0.3941])\n",
      "tensor([   -inf, -0.2258,  0.1021,  0.3286])\n",
      "tensor([0.0000, 0.2422, 0.3362, 0.4216])\n",
      "tensor([   -inf, -0.2632,  0.0893,  0.3035])\n",
      "tensor([0.0000, 0.2389, 0.3399, 0.4211])\n",
      "tensor([   -inf, -0.2823,  0.0378,  0.2811])\n",
      "tensor([0.0000, 0.2419, 0.3332, 0.4249])\n",
      "tensor([   -inf, -0.1867,  0.0261,  0.1893])\n",
      "tensor([0.0000, 0.2708, 0.3349, 0.3943])\n",
      "tensor([   -inf, -0.2476,  0.1056,  0.3246])\n",
      "tensor([0.0000, 0.2383, 0.3393, 0.4224])\n",
      "tensor([   -inf, -0.2585,  0.0930,  0.2944])\n",
      "tensor([0.0000, 0.2404, 0.3417, 0.4179])\n",
      "tensor([   -inf, -0.2742,  0.0720,  0.2931])\n",
      "tensor([0.0000, 0.2394, 0.3384, 0.4222])\n",
      "tensor([   -inf, -0.1812,  0.0329,  0.1928])\n",
      "tensor([0.0000, 0.2708, 0.3355, 0.3937])\n",
      "tensor([   -inf, -0.2433,  0.1068,  0.3262])\n",
      "tensor([0.0000, 0.2389, 0.3390, 0.4221])\n",
      "tensor([   -inf, -0.2615,  0.0895,  0.3024])\n",
      "tensor([0.0000, 0.2394, 0.3400, 0.4207])\n",
      "tensor([   -inf, -0.2784,  0.0595,  0.2899])\n",
      "tensor([0.0000, 0.2400, 0.3364, 0.4236])\n",
      "tensor([   -inf, -0.1845,  0.0304,  0.1926])\n",
      "tensor([0.0000, 0.2704, 0.3353, 0.3943])\n",
      "tensor([   -inf, -0.2495,  0.1075,  0.3225])\n",
      "tensor([0.0000, 0.2380, 0.3402, 0.4218])\n",
      "tensor([   -inf, -0.2574,  0.0933,  0.2933])\n",
      "tensor([0.0000, 0.2407, 0.3418, 0.4175])\n",
      "tensor([   -inf, -0.2740,  0.0761,  0.2961])\n",
      "tensor([0.0000, 0.2388, 0.3389, 0.4223])\n",
      "tensor([   -inf, -0.1791,  0.0357,  0.1918])\n",
      "tensor([0.0000, 0.2711, 0.3361, 0.3928])\n",
      "tensor([   -inf, -0.2391,  0.1059,  0.3245])\n",
      "tensor([0.0000, 0.2399, 0.3387, 0.4214])\n",
      "tensor([   -inf, -0.2604,  0.0892,  0.3015])\n",
      "tensor([0.0000, 0.2397, 0.3400, 0.4204])\n",
      "tensor([   -inf, -0.2787,  0.0574,  0.2882])\n",
      "tensor([0.0000, 0.2403, 0.3362, 0.4235])\n",
      "tensor([   -inf, -0.1864,  0.0239,  0.1852])\n",
      "tensor([0.0000, 0.2714, 0.3350, 0.3936])\n",
      "tensor([   -inf, -0.2555,  0.1071,  0.3224])\n",
      "tensor([0.0000, 0.2370, 0.3406, 0.4224])\n",
      "tensor([   -inf, -0.2586,  0.0939,  0.2957])\n",
      "tensor([0.0000, 0.2402, 0.3417, 0.4181])\n",
      "tensor([   -inf, -0.2784,  0.0577,  0.2875])\n",
      "tensor([0.0000, 0.2403, 0.3364, 0.4233])\n",
      "tensor([   -inf, -0.1810,  0.0342,  0.1920])\n",
      "tensor([0.0000, 0.2708, 0.3359, 0.3933])\n",
      "tensor([   -inf, -0.2533,  0.1055,  0.3242])\n",
      "tensor([0.0000, 0.2374, 0.3398, 0.4229])\n",
      "tensor([   -inf, -0.2582,  0.0929,  0.2953])\n",
      "tensor([0.0000, 0.2404, 0.3415, 0.4181])\n",
      "tensor([   -inf, -0.2783,  0.0683,  0.2941])\n",
      "tensor([0.0000, 0.2388, 0.3378, 0.4234])\n",
      "tensor([   -inf, -0.1783,  0.0359,  0.1930])\n",
      "tensor([0.0000, 0.2711, 0.3359, 0.3930])\n",
      "tensor([   -inf, -0.2753,  0.1109,  0.3164])\n",
      "tensor([0.0000, 0.2337, 0.3439, 0.4224])\n",
      "tensor([   -inf, -0.2587,  0.0920,  0.2959])\n",
      "tensor([0.0000, 0.2403, 0.3413, 0.4184])\n",
      "tensor([   -inf, -0.2700,  0.0911,  0.2984])\n",
      "tensor([0.0000, 0.2381, 0.3416, 0.4203])\n",
      "tensor([   -inf, -0.1753,  0.0410,  0.1956])\n",
      "tensor([0.0000, 0.2710, 0.3364, 0.3926])\n",
      "tensor([   -inf, -0.2584,  0.1082,  0.3224])\n",
      "tensor([0.0000, 0.2364, 0.3411, 0.4225])\n",
      "tensor([   -inf, -0.2594,  0.0908,  0.2976])\n",
      "tensor([0.0000, 0.2401, 0.3408, 0.4191])\n",
      "tensor([   -inf, -0.2760,  0.0798,  0.2975])\n",
      "tensor([0.0000, 0.2380, 0.3397, 0.4223])\n",
      "tensor([   -inf, -0.1807,  0.0360,  0.1928])\n",
      "tensor([0.0000, 0.2707, 0.3361, 0.3932])\n",
      "tensor([   -inf, -0.2636,  0.1097,  0.3190])\n",
      "tensor([0.0000, 0.2357, 0.3423, 0.4220])\n",
      "tensor([   -inf, -0.2600,  0.0932,  0.2958])\n",
      "tensor([0.0000, 0.2400, 0.3417, 0.4184])\n",
      "tensor([   -inf, -0.2770,  0.0687,  0.2919])\n",
      "tensor([0.0000, 0.2393, 0.3381, 0.4227])\n",
      "tensor([   -inf, -0.1803,  0.0342,  0.1937])\n",
      "tensor([0.0000, 0.2708, 0.3356, 0.3936])\n",
      "tensor([   -inf, -0.2734,  0.1103,  0.3165])\n",
      "tensor([0.0000, 0.2341, 0.3436, 0.4223])\n",
      "tensor([   -inf, -0.2587,  0.0935,  0.2920])\n",
      "tensor([0.0000, 0.2406, 0.3422, 0.4173])\n",
      "tensor([   -inf, -0.2734,  0.0803,  0.2972])\n",
      "tensor([0.0000, 0.2385, 0.3396, 0.4219])\n",
      "tensor([   -inf, -0.1804,  0.0367,  0.1951])\n",
      "tensor([0.0000, 0.2704, 0.3360, 0.3936])\n",
      "tensor([   -inf, -0.2763,  0.1107,  0.3139])\n",
      "tensor([0.0000, 0.2338, 0.3443, 0.4219])\n",
      "tensor([   -inf, -0.2570,  0.0968,  0.2867])\n",
      "tensor([0.0000, 0.2412, 0.3435, 0.4153])\n",
      "tensor([   -inf, -0.2696,  0.0947,  0.3009])\n",
      "tensor([0.0000, 0.2376, 0.3420, 0.4204])\n",
      "tensor([   -inf, -0.1739,  0.0466,  0.1974])\n",
      "tensor([0.0000, 0.2705, 0.3373, 0.3922])\n",
      "tensor([   -inf, -0.2791,  0.1114,  0.3131])\n",
      "tensor([0.0000, 0.2333, 0.3448, 0.4218])\n",
      "tensor([   -inf, -0.2543,  0.0985,  0.2793])\n",
      "tensor([0.0000, 0.2422, 0.3447, 0.4130])\n",
      "tensor([   -inf, -0.2645,  0.1102,  0.3056])\n",
      "tensor([0.0000, 0.2368, 0.3444, 0.4188])\n",
      "tensor([   -inf, -0.1705,  0.0506,  0.1968])\n",
      "tensor([0.0000, 0.2709, 0.3379, 0.3911])\n",
      "tensor([   -inf, -0.2772,  0.1114,  0.3134])\n",
      "tensor([0.0000, 0.2336, 0.3446, 0.4218])\n",
      "tensor([   -inf, -0.2517,  0.0975,  0.2823])\n",
      "tensor([0.0000, 0.2425, 0.3438, 0.4137])\n",
      "tensor([   -inf, -0.2657,  0.1035,  0.3029])\n",
      "tensor([0.0000, 0.2374, 0.3434, 0.4192])\n",
      "tensor([   -inf, -0.1722,  0.0461,  0.1974])\n",
      "tensor([0.0000, 0.2709, 0.3370, 0.3921])\n",
      "tensor([   -inf, -0.2771,  0.1128,  0.3131])\n",
      "tensor([0.0000, 0.2336, 0.3450, 0.4215])\n",
      "tensor([   -inf, -0.2546,  0.0957,  0.2871])\n",
      "tensor([0.0000, 0.2416, 0.3430, 0.4154])\n",
      "tensor([   -inf, -0.2696,  0.0950,  0.3012])\n",
      "tensor([0.0000, 0.2375, 0.3421, 0.4204])\n",
      "tensor([   -inf, -0.1718,  0.0499,  0.1978])\n",
      "tensor([0.0000, 0.2706, 0.3378, 0.3916])\n",
      "tensor([   -inf, -0.2705,  0.1106,  0.3157])\n",
      "tensor([0.0000, 0.2347, 0.3436, 0.4217])\n",
      "tensor([   -inf, -0.2553,  0.0951,  0.2859])\n",
      "tensor([0.0000, 0.2417, 0.3431, 0.4152])\n",
      "tensor([   -inf, -0.2681,  0.0983,  0.3015])\n",
      "tensor([0.0000, 0.2375, 0.3426, 0.4198])\n",
      "tensor([   -inf, -0.1721,  0.0461,  0.1967])\n",
      "tensor([0.0000, 0.2710, 0.3371, 0.3919])\n",
      "tensor([   -inf, -0.2691,  0.1109,  0.3153])\n",
      "tensor([0.0000, 0.2349, 0.3436, 0.4215])\n",
      "tensor([   -inf, -0.2543,  0.0960,  0.2846])\n",
      "tensor([0.0000, 0.2419, 0.3434, 0.4147])\n",
      "tensor([   -inf, -0.2693,  0.0992,  0.3009])\n",
      "tensor([0.0000, 0.2373, 0.3430, 0.4197])\n",
      "tensor([   -inf, -0.1736,  0.0462,  0.1977])\n",
      "tensor([0.0000, 0.2706, 0.3371, 0.3923])\n",
      "tensor([   -inf, -0.2600,  0.1099,  0.3186])\n",
      "tensor([0.0000, 0.2363, 0.3421, 0.4215])\n",
      "tensor([   -inf, -0.2577,  0.0940,  0.2948])\n",
      "tensor([0.0000, 0.2404, 0.3418, 0.4178])\n",
      "tensor([   -inf, -0.2736,  0.0735,  0.2924])\n",
      "tensor([0.0000, 0.2395, 0.3388, 0.4217])\n",
      "tensor([   -inf, -0.1775,  0.0347,  0.1923])\n",
      "tensor([0.0000, 0.2715, 0.3356, 0.3929])\n",
      "tensor([   -inf, -0.2704,  0.1103,  0.3167])\n",
      "tensor([0.0000, 0.2346, 0.3433, 0.4221])\n",
      "tensor([   -inf, -0.2553,  0.0959,  0.2878])\n",
      "tensor([0.0000, 0.2414, 0.3430, 0.4156])\n",
      "tensor([   -inf, -0.2710,  0.0920,  0.3011])\n",
      "tensor([0.0000, 0.2375, 0.3415, 0.4209])\n",
      "tensor([   -inf, -0.1720,  0.0460,  0.1960])\n",
      "tensor([0.0000, 0.2711, 0.3372, 0.3917])\n",
      "tensor([   -inf, -0.2814,  0.1118,  0.3128])\n",
      "tensor([0.0000, 0.2329, 0.3451, 0.4219])\n",
      "tensor([   -inf, -0.2536,  0.0984,  0.2823])\n",
      "tensor([0.0000, 0.2421, 0.3442, 0.4137])\n",
      "tensor([   -inf, -0.2674,  0.1017,  0.3014])\n",
      "tensor([0.0000, 0.2374, 0.3434, 0.4193])\n",
      "tensor([   -inf, -0.1695,  0.0496,  0.1959])\n",
      "tensor([0.0000, 0.2713, 0.3378, 0.3910])\n",
      "tensor([   -inf, -0.2629,  0.1074,  0.3201])\n",
      "tensor([0.0000, 0.2359, 0.3416, 0.4225])\n",
      "tensor([   -inf, -0.2543,  0.0956,  0.2872])\n",
      "tensor([0.0000, 0.2417, 0.3429, 0.4154])\n",
      "tensor([   -inf, -0.2724,  0.0880,  0.2992])\n",
      "tensor([0.0000, 0.2378, 0.3410, 0.4212])\n",
      "tensor([   -inf, -0.1738,  0.0438,  0.1936])\n",
      "tensor([0.0000, 0.2712, 0.3371, 0.3916])\n",
      "tensor([   -inf, -0.2678,  0.1108,  0.3183])\n",
      "tensor([0.0000, 0.2349, 0.3430, 0.4221])\n",
      "tensor([   -inf, -0.2542,  0.0970,  0.2843])\n",
      "tensor([0.0000, 0.2419, 0.3437, 0.4144])\n",
      "tensor([   -inf, -0.2707,  0.0961,  0.3017])\n",
      "tensor([0.0000, 0.2372, 0.3423, 0.4205])\n",
      "tensor([   -inf, -0.1750,  0.0477,  0.1957])\n",
      "tensor([0.0000, 0.2704, 0.3379, 0.3917])\n",
      "tensor([   -inf, -0.2779,  0.1099,  0.3143])\n",
      "tensor([0.0000, 0.2336, 0.3442, 0.4222])\n",
      "tensor([   -inf, -0.2537,  0.0961,  0.2832])\n",
      "tensor([0.0000, 0.2422, 0.3436, 0.4143])\n",
      "tensor([   -inf, -0.2660,  0.1072,  0.3052])\n",
      "tensor([0.0000, 0.2368, 0.3439, 0.4192])\n",
      "tensor([   -inf, -0.1729,  0.0482,  0.1966])\n",
      "tensor([0.0000, 0.2707, 0.3377, 0.3917])\n",
      "tensor([   -inf, -0.2714,  0.1100,  0.3183])\n",
      "tensor([0.0000, 0.2343, 0.3431, 0.4226])\n",
      "tensor([   -inf, -0.2540,  0.0986,  0.2827])\n",
      "tensor([0.0000, 0.2419, 0.3442, 0.4138])\n",
      "tensor([   -inf, -0.2686,  0.0933,  0.2997])\n",
      "tensor([0.0000, 0.2380, 0.3418, 0.4202])\n",
      "tensor([   -inf, -0.1751,  0.0452,  0.1963])\n",
      "tensor([0.0000, 0.2706, 0.3372, 0.3922])\n",
      "tensor([   -inf, -0.2792,  0.1108,  0.3162])\n",
      "tensor([0.0000, 0.2331, 0.3442, 0.4227])\n",
      "tensor([   -inf, -0.2567,  0.0971,  0.2871])\n",
      "tensor([0.0000, 0.2411, 0.3435, 0.4154])\n",
      "tensor([   -inf, -0.2725,  0.0881,  0.2993])\n",
      "tensor([0.0000, 0.2378, 0.3410, 0.4212])\n",
      "tensor([   -inf, -0.1769,  0.0444,  0.1956])\n",
      "tensor([0.0000, 0.2703, 0.3373, 0.3924])\n",
      "tensor([   -inf, -0.2734,  0.1125,  0.3160])\n",
      "tensor([0.0000, 0.2340, 0.3442, 0.4219])\n",
      "tensor([   -inf, -0.2573,  0.0949,  0.2908])\n",
      "tensor([0.0000, 0.2408, 0.3425, 0.4166])\n",
      "tensor([   -inf, -0.2688,  0.0855,  0.2978])\n",
      "tensor([0.0000, 0.2388, 0.3404, 0.4208])\n",
      "tensor([   -inf, -0.1776,  0.0400,  0.1943])\n",
      "tensor([0.0000, 0.2707, 0.3365, 0.3927])\n",
      "tensor([   -inf, -0.2744,  0.1116,  0.3171])\n",
      "tensor([0.0000, 0.2338, 0.3439, 0.4223])\n",
      "tensor([   -inf, -0.2562,  0.0954,  0.2889])\n",
      "tensor([0.0000, 0.2412, 0.3428, 0.4160])\n",
      "tensor([   -inf, -0.2684,  0.1026,  0.3042])\n",
      "tensor([0.0000, 0.2369, 0.3432, 0.4199])\n",
      "tensor([   -inf, -0.1759,  0.0450,  0.1933])\n",
      "tensor([0.0000, 0.2707, 0.3376, 0.3916])\n",
      "tensor([   -inf, -0.2504,  0.1062,  0.3220])\n",
      "tensor([0.0000, 0.2380, 0.3400, 0.4219])\n",
      "tensor([   -inf, -0.2588,  0.0930,  0.2943])\n",
      "tensor([0.0000, 0.2404, 0.3417, 0.4179])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Layers: 100%|ââââââââââ| 4/4 [00:08<00:00,  2.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([   -inf, -0.2729,  0.0780,  0.2967])\n",
      "tensor([0.0000, 0.2388, 0.3392, 0.4221])\n",
      "tensor([   -inf, -0.1817,  0.0341,  0.1935])\n",
      "tensor([0.0000, 0.2706, 0.3357, 0.3937])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = OctreeTransformer_pl.load_from_checkpoint(\"lightning_logs/version_0/checkpoints/epoch=1426-step=14270.ckpt\", embed_dim=embed_dim, num_heads=num_heads, dropout=dropout, num_layers=num_layers, num_vocab=NUM_VOCAB, resolution=resolution)\n",
    "# create sampler\n",
    "sampler = Sampler(model,max_resolution=RESOLUTION)\n",
    "# create sample\n",
    "sample = sampler(2,16,1)\n",
    "# save sample as obj\n",
    "save_obj(sample, \"sample\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3bb30e92d804a618d8f430641045cf4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# use k3d to plot the sampled shape\n",
    "plt_voxels = k3d.voxels(sample,\n",
    "                        color_map=[0xfdc192, 0xa15525],\n",
    "                        outlines_color=0xffffff)\n",
    "\n",
    "plot = k3d.plot()\n",
    "plot += plt_voxels\n",
    "plot.display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
